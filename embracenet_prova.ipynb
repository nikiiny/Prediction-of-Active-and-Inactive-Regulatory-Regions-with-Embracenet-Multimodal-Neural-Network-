{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzYWmpjmYPIm"
   },
   "source": [
    "# EMBRACENET APPLICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BH_kpTB8CtJC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class EmbraceNet(nn.Module):\n",
    "  \n",
    "  def __init__(self, device, input_size_list, embracement_size=256, bypass_docking=False):\n",
    "    \"\"\"\n",
    "    Initialize an EmbraceNet module.\n",
    "    Args:\n",
    "      device: A \"torch.device()\" object to allocate internal parameters of the EmbraceNet module.\n",
    "      input_size_list: A list of input sizes.\n",
    "      embracement_size: The length of the output of the embracement layer (\"c\" in the paper).\n",
    "      bypass_docking: Bypass docking step, i.e., connect the input data directly to the embracement layer. If True, input_data must have a shape of [batch_size, embracement_size].\n",
    "    \"\"\"\n",
    "    super(EmbraceNet, self).__init__()\n",
    "\n",
    "    self.device = device\n",
    "    self.input_size_list = input_size_list\n",
    "    self.embracement_size = embracement_size\n",
    "    self.bypass_docking = bypass_docking\n",
    "\n",
    "    if (not bypass_docking):\n",
    "      for i, input_size in enumerate(input_size_list):\n",
    "        setattr(self, 'docking_%d' % (i), nn.Linear(input_size, embracement_size))\n",
    "\n",
    "\n",
    "  def forward(self, input_list, availabilities=None, selection_probabilities=None):\n",
    "    \"\"\"\n",
    "    Forward input data to the EmbraceNet module.\n",
    "    Args:\n",
    "      input_list: A list of input data. Each input data should have a size as in input_size_list.\n",
    "      availabilities: A 2-D tensor of shape [batch_size, num_modalities], which represents the availability of data for each modality. If None, it assumes that data of all modalities are available.\n",
    "      selection_probabilities: A 2-D tensor of shape [batch_size, num_modalities], which represents probabilities that output of each docking layer will be selected (\"p\" in the paper). If None, the same probability of being selected will be used for each docking layer.\n",
    "    Returns:\n",
    "      A 2-D tensor of shape [batch_size, embracement_size] that is the embraced output.\n",
    "    \"\"\"\n",
    "\n",
    "    # check input data\n",
    "    assert len(input_list) == len(self.input_size_list)\n",
    "    num_modalities = len(input_list)\n",
    "    batch_size = input_list[0].shape[0]\n",
    "    \n",
    "\n",
    "    # docking layer\n",
    "    docking_output_list = []\n",
    "    if (self.bypass_docking):\n",
    "      docking_output_list = input_list\n",
    "    else:\n",
    "      for i, input_data in enumerate(input_list):\n",
    "        x = getattr(self, 'docking_%d' % (i))(input_data)\n",
    "        x = nn.functional.relu(x)\n",
    "        docking_output_list.append(x)\n",
    "    \n",
    "\n",
    "    # check availabilities\n",
    "    if (availabilities is None):\n",
    "      availabilities = torch.ones(batch_size, len(input_list), dtype=torch.float, device=self.device)\n",
    "    else:\n",
    "      availabilities = availabilities.float()\n",
    "    \n",
    "\n",
    "    # adjust selection probabilities\n",
    "    if (selection_probabilities is None):\n",
    "      selection_probabilities = torch.ones(batch_size, len(input_list), dtype=torch.float, device=self.device)\n",
    "    selection_probabilities = torch.mul(selection_probabilities, availabilities)\n",
    "\n",
    "    probability_sum = torch.sum(selection_probabilities, dim=-1, keepdim=True)\n",
    "    selection_probabilities = torch.div(selection_probabilities, probability_sum)\n",
    "\n",
    "\n",
    "    # stack docking outputs\n",
    "    docking_output_stack = torch.stack(docking_output_list, dim=-1)  # [batch_size, embracement_size, num_modalities]\n",
    "\n",
    "\n",
    "    # embrace\n",
    "    modality_indices = torch.multinomial(selection_probabilities, num_samples=self.embracement_size, replacement=True)  # [batch_size, embracement_size]\n",
    "    modality_toggles = nn.functional.one_hot(modality_indices, num_classes=num_modalities).float()  # [batch_size, embracement_size, num_modalities]\n",
    "\n",
    "    embracement_output_stack = torch.mul(docking_output_stack, modality_toggles)\n",
    "    embracement_output = torch.sum(embracement_output_stack, dim=-1)  # [batch_size, embracement_size]\n",
    "\n",
    "    return embracement_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "goeL7ZZU--j-"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class VGG_pre(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG_pre, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32), \n",
    "            nn.ReLU(),\n",
    "      #      nn.Conv2d(32, 32, kernel_size=5, stride=1, padding=2),\n",
    "       #     nn.BatchNorm2d(32),\n",
    "        #    nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "     #       nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=2),\n",
    "      #      nn.BatchNorm2d(64),\n",
    "       #     nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.drop_out1 = nn.Dropout(p=0.3)\n",
    "        self.drop_out2 = nn.Dropout(p=0.4)\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "      out = self.layer1(x)\n",
    "      out = self.drop_out1(out)\n",
    "      out = self.layer2(out)\n",
    "      out = self.drop_out2(out)\n",
    "      out = out.reshape(out.size(0), -1) \n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEBTsRqCbZLh"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "training_loader = defaultdict(lambda:0)\n",
    "validation_loader = defaultdict(lambda:0)\n",
    "\n",
    "training_loader['left'] = loader_imag_train_L\n",
    "training_loader['central'] = loader_imag_train_C\n",
    "training_loader['right'] = loader_imag_train_R\n",
    "\n",
    "validation_loader['left'] = loader_imag_validation_L\n",
    "validation_loader['central'] = loader_imag_validation_C\n",
    "validation_loader['right'] = loader_imag_validation_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cenBsd-Xyxhy",
    "outputId": "2fbc2005-7fcb-4ca3-ea14-96b9a571cace"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>>,\n",
       "            {'central': <torch.utils.data.dataloader.DataLoader at 0x7f0810cb6e10>,\n",
       "             'left': <torch.utils.data.dataloader.DataLoader at 0x7f07ffb169d0>,\n",
       "             'right': <torch.utils.data.dataloader.DataLoader at 0x7f07ffb92250>})"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oF9zMxSKYN1n"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class EmbraceNetMultimodal(nn.Module):\n",
    "  def __init__(self, device, n_classes, hyperparameters_tuning=False, args=None):\n",
    "    super(EmbraceNetMultimodal, self).__init__()\n",
    "    \n",
    "    \n",
    "    # input parameters\n",
    "    self.device = device\n",
    "    self.n_classes = n_classes\n",
    "    self.args = args\n",
    "    self.hyperparameters_tuning = hyperparameters_tuning\n",
    "\n",
    "    \n",
    "    # VGG convolutional neural network\n",
    "    self.VGG_left = VGG_pre()\n",
    "    self.VGG_central = VGG_pre()\n",
    "    self.VGG_right = VGG_pre()\n",
    "\n",
    "    # load previously trained models to find optimal hyperparameters\n",
    "    if self.hyperparameters_tuning:\n",
    "      load_model(self.VGG_left, 'best_model_L_hp.pt')\n",
    "      load_model(self.VGG_central, 'best_model_C_hp.pt')\n",
    "      load_model(self.VGG_right, 'best_model_R_hp.pt')\n",
    "    \n",
    "    # load previously trained models for final testing\n",
    "    else:\n",
    "      load_model(self.VGG_left, 'best_model_L_test.pt')\n",
    "      load_model(self.VGG_central, 'best_model_C_test.pt')\n",
    "      load_model(self.VGG_right, 'best_model_R_test.pt')\n",
    "\n",
    "    # freeze layers\n",
    "    for param in self.VGG_left.parameters():\n",
    "      param.requires_grad = False\n",
    "    for param in self.VGG_central.parameters():\n",
    "      param.requires_grad = False\n",
    "    for param in self.VGG_right.parameters():\n",
    "      param.requires_grad = False\n",
    "        \n",
    "    self.pre_output_size = (5*64*16) #5120\n",
    "\n",
    "    # embracenet\n",
    "    self.embracenet = EmbraceNet(device=self.device, \n",
    "                                 input_size_list=[self.pre_output_size, self.pre_output_size, self.pre_output_size], \n",
    "                                 embracement_size=512)\n",
    "\n",
    "    # post embracement layers\n",
    "    self.post = nn.Linear(in_features=512, out_features=self.n_classes)\n",
    "\n",
    "  \n",
    "  def forward(self, x, availabilities=None, selection_probabilities=None):\n",
    "\n",
    "    x_left, x_central, x_right = x\n",
    "\n",
    "    x_left_final = self.VGG_left(x_left)\n",
    "    x_central_final = self.VGG_central(x_central)\n",
    "    x_right_final = self.VGG_right(x_right)\n",
    "\n",
    "    # drop left or right modality\n",
    "   # availabilities = None\n",
    "    #if (self.args.model_drop_left or self.args.model_drop_central or self.args.model_drop_right):\n",
    "   #   availabilities = torch.ones([x.shape[0], 3], device=self.device)\n",
    "    #  if (self.args.model_drop_left):\n",
    "     #   availabilities[:, 0] = 0\n",
    "   #   if (self.args.model_drop_central):\n",
    "    #    availabilities[:, 1] = 0\n",
    "     # if (self.args.model_drop_right):\n",
    "      #  availabilities[:, 2] = 0\n",
    "\n",
    "    # dropout during training\n",
    " #   if (self.is_training and self.args.model_dropout):\n",
    "  #    dropout_prob = torch.rand(1, device=self.device)[0]\n",
    "   #   if (dropout_prob >= 0.5):\n",
    "    #    target_modalities = torch.round(torch.rand([x.shape[0]], device=self.device)).to(torch.int64)\n",
    "     #  availabilities = nn.functional.one_hot(target_modalities, num_classes=2).float()\n",
    "\n",
    "    # embrace\n",
    "    embracenet = self.embracenet([x_left_final, x_central_final, x_right_final]) #, availabilities=availabilities)\n",
    "\n",
    "    # employ final layers\n",
    "    output = self.post(embracenet)\n",
    "\n",
    "    # output softmax\n",
    "    return output\n",
    "    \n",
    "   # return nn.functional.log_softmax(output, dim=-1) #not needed since it's already applied\n",
    "   #by cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6Szz7dYTiYZ"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 50\n",
    "num_classes = 5\n",
    "#learning_rate=0.0001\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
    "\n",
    "#optimizer = torch.optim.Adam([x_left, x_central, x_right],lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atm7JYZTRRWH"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch.nn as nn\n",
    "#import thop\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "class Param_Search_Multimodal():\n",
    "\n",
    "  \"\"\"Performs the hyper parameters tuning by using a TPE (Tree-structured Parzen Estimator) \n",
    "    algorithm sampler.  \n",
    "    \n",
    "    Parameters:\n",
    "    ------------------\n",
    "    model (torch.nn.Module): neural network model.\n",
    "    train_loader (DataLoader): dictionary of training DataLoader objects. Keys of the\n",
    "        dictionary must be 'FFNN', 'CNN', 'D2V_CNN'.\n",
    "    test_loader (DataLoader): dictionary of testing DataLoader objects. Keys of the\n",
    "        dictionary must be 'FFNN', 'CNN', 'D2V_CNN'.\n",
    "    criterion : loss function for training the model.\n",
    "    num_epochs (int): number of epochs.\n",
    "    study_name (str): name of the Optuna study object.\n",
    "    n_trial (int): number of trials to perform in the Optuna study.\n",
    "        Default: 4\n",
    "    \n",
    "    Attributes:\n",
    "    ------------------\n",
    "    best_model: stores the weights of the common layers of the best performing model.\n",
    "    \n",
    "    Returns:\n",
    "    ------------------\n",
    "  Prints values of the optimised hyperparameters and saves the parameters of the best model.\n",
    "    \"\"\"\n",
    "    \n",
    "  def __init__(self, \n",
    "               model, \n",
    "               train_loader, \n",
    "               test_loader,\n",
    "               criterion,\n",
    "               num_epochs,\n",
    "               n_trials,\n",
    "               study_name):\n",
    "    self.model = model\n",
    "    self.train_loader = train_loader\n",
    "    self.test_loader = test_loader\n",
    "    self.criterion = criterion\n",
    "    self.num_epochs = num_epochs\n",
    "    self.n_trials = n_trials\n",
    "    self.study_name = study_name\n",
    "    self.len_train_loader = len(train_loader['FFNN'])\n",
    "    self.len_test_loader = len(test_loader['FFNN'])\n",
    "    self.best_model = None\n",
    "\n",
    "  def objective(self, trial):\n",
    "    \"\"\"Defines the objective to be optimised (F1 test score) and saves\n",
    "    each final model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate the model\n",
    "    model = self.model\n",
    "\n",
    "    # generate the possible optimizers\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\"])\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    \n",
    "    # convert model data type to double\n",
    "    model = model.double()\n",
    "    \n",
    "    # Define the training and testing phases\n",
    "    for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "      train_loss = 0.0\n",
    "      test_loss = 0.0\n",
    "      f1_test = 0.0\n",
    "    \n",
    "      # set the model in training modality\n",
    "      model.train()\n",
    "      for load1, load2 in tqdm(zip(self.train_loader['FFNN'],\n",
    "                                          self.train_loader['CNN'])) \n",
    "                                      desc='Training model', total = self.len_train_loader):\n",
    "        x_1, target = load1\n",
    "        x_2, _ = load2\n",
    "\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model([x_1.double(), x_2.double()])\n",
    "        # calculate the batch loss as a sum of the single losses\n",
    "        loss = self.criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss wrt model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()      \n",
    "        \n",
    "        \n",
    "      # set the model in testing modality\n",
    "      model.eval()\n",
    "      for load1, load2 in tqdm(zip(self.test_loader['FFNN'],\n",
    "                                          self.test_loader['CNN']), \n",
    "                                      desc='Testing model', total = self.len_test_loader):\n",
    "        x_1, target = load1\n",
    "        x_2, _, = load2\n",
    "\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output1, output2 = model([x_1.double(), x_2.double()])\n",
    "        # calculate the batch loss as a sum of the single losses\n",
    "        loss = self.criterion(output, target)\n",
    "        # update test loss \n",
    "        test_loss += loss.item() \n",
    "        # calculate F1 test score as weighted sum of the single F1 scores\n",
    "        f1_test += F1(output,target) \n",
    "        \n",
    "      # calculate epoch score by dividing by the number of observations\n",
    "      f1_test /= self.len_test_loader\n",
    "      # pass the score of the epoch to the study to monitor the intermediate objective values    \n",
    "      trial.report(f1_test, epoch)\n",
    "\n",
    "    # save the final model named with the number of the trial \n",
    "    with open(\"{}{}.pickle\".format(self.study_name,trial.number), \"wb\") as fout:\n",
    "      pickle.dump(model, fout)\n",
    "    \n",
    "    # return F1 score to the study        \n",
    "    return f1_test\n",
    "\n",
    "\n",
    "\n",
    "  def run_trial(self):\n",
    "    \"\"\"Runs Optuna study and stores the best model in class attribute 'best_model'.\"\"\"\n",
    "\n",
    "    # create a new study or load a pre-existing study. use sqlite backend to store the study.\n",
    "    study = optuna.create_study(study_name=self.study_name, direction=\"maximize\", \n",
    "                                storage='sqlite:///SA_optuna_tuning.db', load_if_exists=True)\n",
    "\n",
    "    complete_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.COMPLETE]\n",
    "    pruned_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.PRUNED]\n",
    "    \n",
    "    # if the number of already completed trials is lower than the total number of trials passed as\n",
    "    #argument, perform the remaining trials \n",
    "    if len(complete_trials)<self.n_trials:\n",
    "        # set the number of trials to be performed equal to the number of missing trials\n",
    "        self.n_trials -= len(complete_trials)\n",
    "        study.optimize(self.objective, n_trials=self.n_trials)\n",
    "        pruned_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.PRUNED]\n",
    "        complete_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.COMPLETE]\n",
    "        \n",
    "    # store the best model found in the class\n",
    "    with open(\"{}{}.pickle\".format(self.study_name, study.best_trial.number), \"rb\") as fin:\n",
    "        best_model = pickle.load(fin)\n",
    "\n",
    "    self.best_model = best_model\n",
    "    \n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "      print(\"    {}: {}\".format(key, value))\n",
    "                                          \n",
    "\n",
    "    with open(\"{}.pickle\".format(study.best_trial.number), \"rb\") as fin:\n",
    "      best_model = pickle.load(fin)\n",
    "    \n",
    "    # store only best model\n",
    "    self.best_model = best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwpyKxqQexj1"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def fit_multimodal(model, \n",
    "                   train_loader, \n",
    "                   test_loader, \n",
    "                   criterion, \n",
    "                   optimizer, \n",
    "                   num_epochs, \n",
    "                   filename_path, \n",
    "                   verbose=True): \n",
    "  \"\"\"Performs the training of the multitask model. It implements also early stopping\n",
    "    \n",
    "    Parameters:\n",
    "    ------------------\n",
    "    model (torch.nn.Module): neural network model.\n",
    "    train_loader (DataLoader): dictioary of training DataLoader objects. Keys of the\n",
    "        dictionary must be 'FFNN', 'CNN', 'D2V_CNN'.\n",
    "    test_loader (DataLoader): dictionary of testing DataLoader objects. Keys of the\n",
    "        dictionary must be 'FFNN', 'CNN', 'D2V_CNN'.\n",
    "    criterion: loss function for training the model.\n",
    "    optimizer (torch.optim): optimization algorithm for training the model. \n",
    "    num_epochs (int): number of epochs.\n",
    "    filename_path (str): where the weights of the model at each epoch will be stored. \n",
    "        Indicate only the name of the folder.\n",
    "    patience (int): number of epochs in which the test error is not anymore decreasing\n",
    "        before stopping the training.\n",
    "    delta (int): minimum decrease in the test error to continue with the training.\n",
    "        Default:0\n",
    "    verbose (bool): prints the training error, test error, F1 training score, F1 test score \n",
    "        at each epoch.\n",
    "        Default: True\n",
    "    \n",
    "    Attributes:\n",
    "    ------------------\n",
    "    f1_train_scores: stores the F1 training scores for each epoch.\n",
    "    f1_test_scores: stores the F1 test scores for each epoch.\n",
    "    \n",
    "    Returns:\n",
    "    ------------------\n",
    "    Lists of F1 training scores and F1 test scores at each epoch.\n",
    "    Prints training error, test error, F1 training score, F1 test score at each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "  basepath = 'exp'\n",
    "\n",
    "  # keep track of epoch losses \n",
    "  f1_train_scores = []\n",
    "  f1_test_scores = []\n",
    "\n",
    "  # convert model data type to double\n",
    "  model = model.double()\n",
    "\n",
    "  # define early stopping\n",
    "  early_stopping = EarlyStopping(patience=patience, delta=delta, verbose=True)\n",
    "\n",
    "  len_train_loader = len(train_loader['FFNN'])\n",
    "  len_test_loader = len(test_loader['FFNN'])\n",
    "    \n",
    "\n",
    "  for epoch in tqdm(range(1, num_epochs + 1), desc='Epochs'):\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    f1_train = 0.0\n",
    "    f1_test = 0.0\n",
    "    \n",
    "    # if there is already a trained model stored for a specific epoch, load the model\n",
    "    #and don't retrain the model\n",
    "    PATH = os.path.join(basepath, filename_path + '_' + str(epoch) + '.pt')\n",
    "    if os.path.exists(PATH):\n",
    "      checkpoint = torch.load(PATH)\n",
    "      model.load_state_dict(checkpoint['model_state_dict'])\n",
    "      f1_train = checkpoint['F1_train']\n",
    "      f1_test = checkpoint['F1_test']\n",
    "      train_loss = checkpoint['train_loss']\n",
    "      test_loss = checkpoint['test_loss']\n",
    "        \n",
    "    else:\n",
    "      # set the model in training modality\n",
    "      model.train()\n",
    "      for load1, load2, load3 in tqdm(zip(train_loader['FFNN'],\n",
    "                                          train_loader['CNN']), \n",
    "                                      desc='Testing model', total = len_train_loader):\n",
    "        x_1, target = load1\n",
    "        x_2, _ = load2\n",
    "        x_3, _ = load3\n",
    "        \n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model([x_1.double(), x_2.double()])\n",
    "        # calculate the batch loss as the sum of all the losses\n",
    "        loss = criterion(output1 target) \n",
    "        # backward pass: compute gradient of the loss wrt model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()\n",
    "        # calculate F1 training score as a weighted sum of the single F1 scores\n",
    "        f1_train +=  F1(output,target) \n",
    "        \n",
    "        \n",
    "      # set the model in testing modality\n",
    "      model.eval()\n",
    "      for load1, load2, load3 in tqdm(zip(test_loader['FFNN'],\n",
    "                                          test_loader['CNN']), \n",
    "                                      desc='Testing model', total = len_test_loader):\n",
    "        x_1, target = load1\n",
    "        x_2, _ = load2\n",
    "        x_3, _ = load3\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model([x_1.double(), x_2.double()])\n",
    "        # calculate the batch loss as the sum of all the losses\n",
    "        loss = criterion(output, target) \n",
    "        # update test loss\n",
    "        test_loss += loss.item()\n",
    "        # calculate F1 test score as a weighted sum of the single F1 scores\n",
    "        f1_test +=  F1(output,target) \n",
    "        \n",
    "        \n",
    "    # save the model weights, epoch, scores and losses at each epoch\n",
    "    model_param = model.state_dict()\n",
    "    PATH = os.path.join(basepath, filename_path + '_' + str(epoch) + '.pt')\n",
    "    torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model_param,\n",
    "                'F1_train': f1_train,\n",
    "                'F1_test': f1_test,\n",
    "                'train_loss': train_loss,\n",
    "                'test_loss': test_loss},\n",
    "               PATH)\n",
    "     \n",
    "    \n",
    "    # calculate epoch score by dividing by the number of observations\n",
    "    f1_train /= len_train_loader\n",
    "    f1_test /= len_test_loader\n",
    "    # store epoch scores\n",
    "    f1_train_scores.append(f1_train)    \n",
    "    f1_test_scores.append(f1_test)\n",
    "      \n",
    "    # print training/test statistics \n",
    "    if verbose == True:\n",
    "      print('Epoch: {} \\tTraining F1 score: {:.4f} \\tTest F1 score: {:.4f} \\tTraining Loss: {:.4f} \\tTest Loss: {:.4f}'.format(\n",
    "      epoch, f1_train, f1_test, train_loss, test_loss))\n",
    "      \n",
    "    # early stop the model if the test loss is not improving\n",
    "    early_stopping(test_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "      print('Early stopping the training')\n",
    "      # reload the previous best model before the test loss started decreasing\n",
    "      best_checkpoint = torch.load(os.path.join(basepath,filename_path + '_' + '{}'.format(epoch-patience) + '.pt'))\n",
    "      model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "      break\n",
    "    \n",
    "        \n",
    "  # return the scores at each epoch\n",
    "  return f1_train_scores, f1_test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtReJYQW7bBj"
   },
   "source": [
    "## Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDe4BnP_7eEw"
   },
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
