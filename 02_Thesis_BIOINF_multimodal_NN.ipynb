{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqpqu8UJFySp"
   },
   "source": [
    "## BIOINFORMATICS THESIS: MULTIMODAL NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EMCr7Kmoywoa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Niki/opt/anaconda3/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import gensim\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import types\n",
    "\n",
    "#import optuna\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import re\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uuhDdx1hy5QF",
    "outputId": "3c0936ff-f819-4c28-99db-d772ac7856c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (2.7.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from optuna) (1.3.20)\n",
      "Requirement already satisfied: scipy!=1.4.0 in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from optuna) (1.5.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from optuna) (20.4)\n",
      "Requirement already satisfied: numpy in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from optuna) (1.19.2)\n",
      "Requirement already satisfied: tqdm in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from optuna) (4.50.2)\n",
      "Requirement already satisfied: colorlog in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from optuna) (5.0.1)\n",
      "Requirement already satisfied: cliff in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from optuna) (3.7.0)\n",
      "Requirement already satisfied: alembic in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from optuna) (1.6.2)\n",
      "Requirement already satisfied: cmaes>=0.8.2 in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from optuna) (0.8.2)\n",
      "Requirement already satisfied: six in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->optuna) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->optuna) (2.4.7)\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from cliff->optuna) (2.1.0)\n",
      "Requirement already satisfied: cmd2>=1.0.0 in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from cliff->optuna) (1.5.0)\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from cliff->optuna) (5.6.0)\n",
      "Requirement already satisfied: stevedore>=2.0.1 in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from cliff->optuna) (3.3.0)\n",
      "Requirement already satisfied: PyYAML>=3.12 in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from cliff->optuna) (5.3.1)\n",
      "Requirement already satisfied: python-dateutil in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from alembic->optuna) (2.8.1)\n",
      "Requirement already satisfied: Mako in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from alembic->optuna) (1.1.4)\n",
      "Requirement already satisfied: python-editor>=0.3 in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from alembic->optuna) (1.0.4)\n",
      "Requirement already satisfied: wcwidth in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from PrettyTable>=0.7.2->cliff->optuna) (0.2.5)\n",
      "Requirement already satisfied: attrs>=16.3.0 in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna) (20.3.0)\n",
      "Requirement already satisfied: pyperclip>=1.6 in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
      "Requirement already satisfied: colorama>=0.3.7 in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from Mako->alembic->optuna) (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "dSHwzDMJsdRF"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import itertools\n",
    "from scipy.stats import pointbiserialr\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import average_precision_score\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import entropy\n",
    "import optuna\n",
    "\n",
    "class Data_Preparation():\n",
    "    \n",
    "    def __init__(self, data_dict, n_neighbors=5):\n",
    "        \n",
    "        self.label = data_dict['bed'].drop(['chrom','chromStart','chromEnd','strand'], axis=1)\n",
    "        self.index = data_dict['bed'][['chrom','chromStart','chromEnd','strand']]\n",
    "        self.data_dict = data_dict\n",
    "        del self.data_dict['bed']\n",
    "        \n",
    "        self.data_dict['fa'] = self.data_dict['fa']['chromosome']\n",
    "        \n",
    "        # drop observations info\n",
    "        for key in self.data_dict.keys():\n",
    "            if key != 'fa':\n",
    "                self.data_dict[key] = self.data_dict[key].drop(['chrom','chromStart','chromEnd','strand'], axis=1)\n",
    "                \n",
    "        \n",
    "        self.n_neighbors = n_neighbors\n",
    "        \n",
    "        self.robust_scaler = RobustScaler()\n",
    "      #  self.label_encoder = LabelEncoder()\n",
    "       # self.onehot_encoder = OneHotEncoder(sparse=False) \n",
    "        self.knn_imputer = KNNImputer(n_neighbors=self.n_neighbors)\n",
    "        \n",
    "        self.X_train = []\n",
    "        self.y_train = []\n",
    "        self.X_test = []\n",
    "        self.y_test = []\n",
    "        \n",
    "        self.sequence = []\n",
    "        \n",
    "        self.to_drop = dict()\n",
    "    \n",
    "                    \n",
    "    def scale_data_genfeatures(self):\n",
    "        for key in self.data_dict.keys():\n",
    "            if key != 'fa':\n",
    "                self.data_dict[key] = pd.DataFrame(self.robust_scaler.fit_transform(self.data_dict[key].values),\n",
    "                                                   index=self.data_dict[key].index, \n",
    "                                                   columns=self.data_dict[key].columns)\n",
    "                \n",
    "    \n",
    "    def knn_imputation_genfeatures(self):\n",
    "        for key in self.data_dict.keys():\n",
    "            if key != 'fa':\n",
    "                self.data_dict[key] = pd.DataFrame(self.knn_imputer.fit_transform(self.data_dict[key].values),\n",
    "                                                   index=self.data_dict[key].index, \n",
    "                                                   columns=self.data_dict[key].columns)\n",
    "    \n",
    "    \n",
    "     # TRANSFORM WHEN LOADING INTO NETWORK\n",
    "  #  def knn_imputation_sequence_fit(self):\n",
    "   #     self.label_encoder.fit(np.array(['a','c','g','n','t']))\n",
    "        \n",
    "        # encode data since imputer works only with float\n",
    "        #for i,seq in enumerate(self.data_dict['fa']):\n",
    "         #   seq = [i for i in seq.lower()] \n",
    "          #  seq = self.label_encoder.transform(seq)\n",
    "#            seq = [np.nan if i ==3 else i for i in seq]\n",
    " #           seq = self.knn_imputer.fit_transform(self.reshape(seq)).astype(int).round()\n",
    "  #          self.data_dict['fa'][i] = seq\n",
    "                \n",
    " #   def onehot_encode_data_sequence_fit(self):\n",
    "  #      self.onehot_encoder.fit(np.array([0,1,2,4]).reshape(-1, 1)) \n",
    "        \n",
    "#        for i,seq in enumerate(self.data_dict['fa']):\n",
    " #           self.data_dict['fa'][i] = self.onehot_encoder.transform(seq)\n",
    "            \n",
    "  #      self.data_dict['fa'] = torch.tensor(self.data_dict['fa'])\n",
    "    \n",
    "    def transform(self):\n",
    "        \n",
    "        self.scale_data_genfeatures()\n",
    "        self.knn_imputation_genfeatures()\n",
    "        \n",
    "     #   self.knn_imputation_sequence()\n",
    "      #  self.onehot_encode_data_sequence()\n",
    "        \n",
    "  \n",
    "\n",
    "    def point_biserial_corr(self, X, y, verbose=False):\n",
    "        \"\"\"Point biserial correlation returns the correlation between a continuous and\n",
    "        binary variable (target). It is a parametric test, so it assumes the data to be normally\n",
    "        distributed.\"\"\"\n",
    "        \n",
    "        uncorrelated = set()\n",
    "\n",
    "        for col in X.columns:\n",
    "            x = X[col]\n",
    "            corr,_ = pointbiserialr(x,y)\n",
    "            if abs(corr) < 0.05:\n",
    "                uncorrelated.add(col)\n",
    "                if verbose:\n",
    "                    print('column: {}, Point-biserial Correlation: {}'.format(col, round(corr,4)))\n",
    "        \n",
    "        return uncorrelated\n",
    "    \n",
    "    \n",
    "    def logistic_regression_corr(self, X, y, verbose=False):\n",
    "        \"\"\"The Logistic regression can be used to assess if a continuous variable has any \n",
    "         effect on the target. It doesn't assume anything about the distribution of the variables.\n",
    "         The metric used is the area under the precision recall curve, which \"\"\"\n",
    "        \n",
    "        uncorrelated = set()\n",
    "        \n",
    "        for col in X.columns:\n",
    "            x = X[col].values.reshape(-1, 1)\n",
    "\n",
    "            # perform 3-folds cv with logistic regression\n",
    "            cv = KFold(n_splits=3, random_state=123, shuffle=True)\n",
    "            model = LogisticRegression()\n",
    "\n",
    "            baseline_binary_pos = len(y[y==1]) / len(y)\n",
    "\n",
    "            # compute the AUPRC for the positive score\n",
    "            AUPRC = make_scorer(average_precision_score, average='weighted')\n",
    "            scores = cross_val_score(model, x, y, scoring=AUPRC, cv=cv, n_jobs=-1, error_score=\"raise\")\n",
    "        \n",
    "            if scores.mean() <= baseline_binary_pos:\n",
    "                uncorrelated.add(col)\n",
    "                \n",
    "                if verbose:\n",
    "                    print('column: {}, AUPRC: {}, Baseline positive class: {}'.format(col, round(scores.mean(),4), round(baseline_binary_pos,4)))\n",
    "        \n",
    "        return uncorrelated\n",
    "\n",
    "    \n",
    "    def correlation_with_label(self, type_corr='all', verbose=False):\n",
    "        \n",
    "        if type_corr not in ['point_biserial_corr', 'logistic_regression', 'all']:\n",
    "            raise ValueError(\n",
    "            \"Argument 'type_corr' has an incorrect value: use 'point_biserial_corr', 'logistic_regression', 'all'\")\n",
    "            \n",
    "        \n",
    "        if type_corr == 'logistic_regression':\n",
    "            for key in self.data_dict.keys():\n",
    "                if key != 'fa':\n",
    "                    if verbose:\n",
    "                        print(key)\n",
    "                    self.to_drop[key] = self.logistic_regression_corr(self.data_dict[key], self.label[key], verbose=verbose)\n",
    "                    # remove uncorrelated features\n",
    "                    if self.to_drop[key]:\n",
    "                        self.data_dict[key] = self.data_dict[key].drop(list(self.to_drop[key]), axis=1)\n",
    "        \n",
    "        \n",
    "        elif type_corr == 'point_biserial_corr':    \n",
    "            for key in self.data_dict.keys():\n",
    "                if key != 'fa':\n",
    "                    if verbose:\n",
    "                        print(key)\n",
    "                    self.to_drop[key] = self.point_biserial_corr(self.data_dict[key], self.label[key], verbose=verbose)\n",
    "                    # remove uncorrelated features\n",
    "                    if self.to_drop[key]:\n",
    "                        self.data_dict[key] = self.data_dict[key].drop(list(self.to_drop[key]), axis=1)\n",
    "        \n",
    "        \n",
    "        elif type_corr == 'all':\n",
    "             for key in self.data_dict.keys():\n",
    "                if key != 'fa':\n",
    "                    if verbose:\n",
    "                        print(key)\n",
    "                    self.to_drop[key] = self.point_biserial_corr(self.data_dict[key], self.label[key], verbose=verbose)\n",
    "                    self.to_drop[key].intersection(self.logistic_regression_corr(self.data_dict[key], self.label[key], verbose=verbose))\n",
    "                    # remove uncorrelated features\n",
    "                    if self.to_drop[key]:\n",
    "                        self.data_dict[key] = self.data_dict[key].drop(list(self.to_drop[key]), axis=1)\n",
    "        \n",
    "    \n",
    "    \n",
    "    def spearman_corr(self, X, verbose=False):\n",
    "        \"\"\"Spearman correlation checks for linear correlation between continuous features.\n",
    "        It is non-parametric, so normality of the variables is not necessary.\"\"\"\n",
    "        \n",
    "        correlated = set()\n",
    "        \n",
    "        for col1, col2 in itertools.combinations(X.columns, 2):\n",
    "            corr, _ = spearmanr(X[col1].values, X[col2].values)\n",
    "            \n",
    "            if corr >= 0.85:\n",
    "                correlated.add(frozenset({col1,col2}))\n",
    "                \n",
    "                if verbose:\n",
    "                    print('correlated columns: {} - {}, Spearman Correlation {}'.format(col1, col2, round(corr,4)))\n",
    "        \n",
    "        return correlated\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def remove_correlated_feature(self, X, y, correlated_pairs, verbose=False):\n",
    "\n",
    "        correlated_to_remove = set()\n",
    "\n",
    "        for pair in correlated_pairs:\n",
    "            col1, col2 = pair\n",
    "            x1 = X[col1].values.reshape(-1, 1)\n",
    "            x2 = X[col2].values.reshape(-1, 1)\n",
    "\n",
    "            # perform 3-folds cv with logistic regression\n",
    "            cv = KFold(n_splits=3, random_state=123, shuffle=True)\n",
    "            model = LogisticRegression()\n",
    "\n",
    "            # compute the AUPRC for the positive score\n",
    "            AUPRC = make_scorer(average_precision_score, average='weighted')\n",
    "            scores1 = cross_val_score(model, x1, y, scoring=AUPRC, cv=cv, n_jobs=-1, error_score=\"raise\")\n",
    "            scores2 = cross_val_score(model, x1, y, scoring=AUPRC, cv=cv, n_jobs=-1, error_score=\"raise\")\n",
    "\n",
    "            if verbose:\n",
    "                print('columns to compare: {} vs {}, AUPRC: {} vs {}'.format(col1, col2, scores1.mean(), scores2.mean()))\n",
    "\n",
    "            if scores1.mean() >= scores2.mean():\n",
    "                correlated_to_remove.add(col2)\n",
    "            else:\n",
    "                correlated_to_remove.add(col1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def correlation_btw_features(self, verbose=False):\n",
    "        \n",
    "        self.to_drop = dict()\n",
    "        \n",
    "        for key in self.data_dict.keys():\n",
    "                if key != 'fa':\n",
    "                    if verbose:\n",
    "                        print(key)\n",
    "                    correlated_pairs = self.spearman_corr(self.data_dict[key], verbose=verbose)\n",
    "                    self.to_drop[key] =  self.remove_correlated_feature(self.data_dict[key], self.label[key], correlated_pairs, verbose=verbose)\n",
    "                    # for each pair of correlated features, remove the one less correlated to the output\n",
    "                    if self.to_drop[key]:\n",
    "                        self.data_dict[key] = self.data_dict[key].drop(list(self.to_drop[key]), axis=1)\n",
    "         \n",
    "    \n",
    "    def split_data(self, cell_line, test_size, validation_size):\n",
    "        \n",
    "        if self.sequence:\n",
    "            assert (self.data_dict['fa'].shape[0] ==  len(self.label[cell_line]))\n",
    "                    \n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.data_dict['fa'], \n",
    "                                                                                 self.label[cell_line],\n",
    "                                                                                 test_size=test_size, \n",
    "                                                                                 random_state=456, shuffle=True) \n",
    "\n",
    "            if self.hyper_tuning:\n",
    "                assert (self.X_train.shape[0] ==  len(self.y_train))\n",
    "                    \n",
    "                self.X_train, self.X_test,  self.y_train, self.y_test = train_test_split(self.X_train, \n",
    "                                                                                     self.y_train,\n",
    "                                                                                     test_size=validation_size,\n",
    "                                                                                     random_state=123, shuffle=True) \n",
    "                   \n",
    "            \n",
    "        else:\n",
    "            assert (self.data_dict[cell_line].shape[0] ==  len(self.label[cell_line]))\n",
    "\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.data_dict[cell_line], \n",
    "                                                                                    self.label[cell_line],\n",
    "                                                                                    test_size=test_size, \n",
    "                                                                                    random_state=123, shuffle=True) \n",
    "            \n",
    "            if self.hyper_tuning: \n",
    "                assert (self.X_train.shape[0] ==  len(self.y_train))\n",
    "\n",
    "                self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X_train, \n",
    "                                                                                        self.y_train,\n",
    "                                                                                        test_size=validation_size, \n",
    "                                                                                        random_state=456, shuffle=True) \n",
    "        \n",
    "    \n",
    "    def return_data(self, cell_line, hyper_tuning=False, sequence=True, test_size=0.25, validation_size=0.15):\n",
    "        \n",
    "      #  for key in self.data_dict.keys():\n",
    "       #     if key != 'fa':\n",
    "        #        if isinstance(self.data_dict[key], pd.DataFrame):\n",
    "         #           self.data_dict[key] = torch.tensor(self.data_dict[key].values)\n",
    "    \n",
    "        self.sequence=sequence\n",
    "        self.hyper_tuning=hyper_tuning\n",
    "        \n",
    "        if cell_line not in ['A549','GM12878', 'H1', 'HEK293', 'HEPG2', 'K562', 'MCF7']:\n",
    "            raise ValueError(\n",
    "            \"Argument 'cell_line' has an incorrect value: use 'A549', 'GM12878', 'H1', 'HEK293', 'HEPG2', 'K562', 'MCF7'\")\n",
    "            \n",
    "        \n",
    "        self.split_data(cell_line=cell_line, test_size=test_size, validation_size=validation_size)\n",
    "    \n",
    "\n",
    "        return ( self.X_train.reset_index(drop=True), self.X_test.reset_index(drop=True), \n",
    "                self.y_train.reset_index(drop=True) , self.y_test.reset_index(drop=True),\n",
    "                self.index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6drgwoRHtkpL"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class Dataset_Wrap(Dataset):\n",
    "    def __init__(self, X, y, n_neighbors, sequence=False):\n",
    "        super(Dataset_Wrap, self).__init__()\n",
    "        \n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.sequence = sequence\n",
    "        \n",
    "        self.pos_index = list(self.X[self.y==1].index)\n",
    "        self.neg_index = list(self.X[self.y==0].index)\n",
    "        \n",
    "        # some observations don't have some of the nucleotides, so it will result in\n",
    "        #matrices of different dimensions which cannot be concatenated\n",
    "        self.label_encoder = LabelEncoder().fit(np.array(['a','c','g','n','t']))\n",
    "        # get rid of value 3 which is 'n' (nan)\n",
    "        self.onehot_encoder = OneHotEncoder(sparse=False).fit(np.array([0,1,2,4]).reshape(-1, 1)) \n",
    "        self.knn_imputer = KNNImputer(n_neighbors=self.n_neighbors)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.X.shape[0])  \n",
    "\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "\n",
    "        data = self.X.iloc[i]  \n",
    "        \n",
    "        if self.sequence:\n",
    "            # all the letters in lowercase\n",
    "            data = [i for i in data.lower()]\n",
    "            \n",
    "            # apply encoding\n",
    "            data = self.label_encoder.transform(data)\n",
    "            # value 3 corresponds to n (nan)\n",
    "            data = [np.nan if i ==3 else i for i in data]\n",
    "            \n",
    "            # impute missing data with knn\n",
    "            data =  self.knn_imputer.fit_transform( np.array(data).reshape(-1,1) ).astype(int).round()\n",
    "            \n",
    "            # one hot encode data\n",
    "          #  data = self.onehot_encoder.transform(data)\n",
    "        \n",
    "        data = torch.tensor(data)\n",
    "            \n",
    "       # if self.sequence: \n",
    "            # (channels, size of the matrix)\n",
    "        #    data = data.reshape(1, data.shape[0], data.shape[1])\n",
    "\n",
    "        label = torch.tensor([self.y[i].astype(int)]).reshape(-1)\n",
    "\n",
    "        return data.to(device), label.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YEXq0cXttn31"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Sampler\n",
    "import random\n",
    "\n",
    "class BalancePos_BatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        \n",
    "        self.pos_index = dataset.pos_index\n",
    "        self.neg_index = dataset.neg_index\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        if len(dataset) % self.batch_size >0:\n",
    "            self.n_batches = (len(dataset) // self.batch_size) +1\n",
    "        else:\n",
    "            self.n_batches = len(dataset) // self.batch_size\n",
    "    \n",
    "    def __iter__(self):\n",
    "        random.shuffle(self.pos_index)\n",
    "        random.shuffle(self.neg_index)\n",
    "        \n",
    "        # create chunks of positive and negative labels\n",
    "        # chunks of positive labels\n",
    "        pos_batches  = np.array_split(self.pos_index, self.n_batches+1)\n",
    "        # chunks of negative labels\n",
    "        neg_batches = np.array_split(self.neg_index, self.n_batches+1)\n",
    "        neg_batches.reverse()\n",
    "        \n",
    "        # create batches of size 100 with the same number of positive obs. in this way we are guaranteed to have\n",
    "        #evenly distributed positive observations across all the batches, so we can always apply SMOTE \n",
    "        balanced = [ np.concatenate((p_batch, n_batch)).tolist() for p_batch, n_batch in zip(pos_batches, neg_batches) ]\n",
    "        random.shuffle(balanced)\n",
    "        return iter(balanced)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "2VOUHCn8pSXl"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "class Build_DataLoader_Pipeline():\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_dict, \n",
    "                 path_name=None,\n",
    "                 n_neighbors=5,\n",
    "                 type_corr='all',\n",
    "                 verbose=False):\n",
    "    \n",
    "        self.data_dict =  data_dict\n",
    "        self.path_name = path_name\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.type_corr = type_corr\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.data_class = []\n",
    "        self.transform = False\n",
    "        self.correlation_with_label = False\n",
    "        self.correlation_btw_features = False\n",
    "        \n",
    "        self.index = []\n",
    "    \n",
    "        \n",
    "        # if there exists already the class of preprocessed data, load it\n",
    "        # else instantiate it and do all the preprocessing\n",
    "        if os.path.exists(\"data_preparation_class_{}\".format(self.path_name)):\n",
    "            with open(\"data_preparation_class_{}\".format(self.path_name), \"rb\") as fin:\n",
    "                self.data_class = pickle.load(fin)\n",
    "        else:\n",
    "            self.data_class = Data_Preparation(self.data_dict, n_neighbors=self.n_neighbors)\n",
    "            self.data_class.transform()\n",
    "            print('Data transformation Done!\\n')\n",
    "          #  self.data_class.correlation_with_label(type_corr=self.type_corr, verbose=self.verbose)\n",
    "           # print('Check correlation with labels Done!\\n')\n",
    "        #    self.data_class.correlation_btw_features(verbose=self.verbose)  \n",
    "         #   print('Check correlation between features Done!\\n')  \n",
    "            \n",
    "          #  with open(\"data_preparation_class_{}\".format(self.path_name), \"wb\") as fout:\n",
    "           #     pickle.dump(self.data_class, fout)\n",
    "        \n",
    "        print('Data Preprocessing Done!')\n",
    "            \n",
    "        \n",
    "    def return_data(self, \n",
    "                    cell_line, \n",
    "                    hyper_tuning=False, \n",
    "                    sequence=False,\n",
    "                    test_size=0.25, \n",
    "                    validation_size=0.15,\n",
    "                    batch_size = 100):\n",
    "            \n",
    "        # retrieve the data \n",
    "        X_train, X_test, y_train, y_test, index = self.data_class.return_data(cell_line=cell_line, \n",
    "                                                                          hyper_tuning=hyper_tuning, \n",
    "                                                                          sequence=sequence,\n",
    "                                                                          test_size=test_size,\n",
    "                                                                          validation_size=validation_size)\n",
    "        \n",
    "        self.index = index\n",
    "        \n",
    "        train_wrap = Dataset_Wrap(X_train, y_train, sequence=sequence, n_neighbors=self.n_neighbors)\n",
    "        test_wrap = Dataset_Wrap(X_test, y_test, sequence=sequence, n_neighbors=self.n_neighbors)\n",
    "        \n",
    "        loader_train = DataLoader(dataset = train_wrap, \n",
    "                                  batch_sampler = BalancePos_BatchSampler(train_wrap, batch_size= batch_size))\n",
    "        loader_test = DataLoader(dataset = test_wrap, \n",
    "                                 batch_sampler = BalancePos_BatchSampler(test_wrap, batch_size= batch_size*2)) \n",
    "\n",
    "        return  loader_train, loader_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OfIwgQITowfc",
    "outputId": "2c4f9784-3f5d-47e4-9163-41424a013cd0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator RobustScaler from version 0.24.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/usr/local/lib/python3.7/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator KNNImputer from version 0.24.2 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('/content/gdrive/MyDrive/Thesis_BIOINF/enhancers_prova_pipe_data_load.pickle', \"rb\") as fin:\n",
    "  pipe_data_load = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enhancers_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-2f16f9b0266f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipe_data_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBuild_DataLoader_Pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menhancers_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'enhancers.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'enhancers_dict' is not defined"
     ]
    }
   ],
   "source": [
    "pipe_data_load = Build_DataLoader_Pipeline(enhancers_dict, path_name='enhancers.pickle', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-428983f654c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'enhancers_prova_pipe_data_load.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mpipe_data_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open('enhancers_prova_pipe_data_load.pickle', \"rb\") as fin:\n",
    "  pipe_data_load = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "open('enhancers_prova_pipe_data_load.pickle', 'a').close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A75bgOrvkxvH"
   },
   "source": [
    "## FUNCTIONS SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lPNL78PBy6HM"
   },
   "outputs": [],
   "source": [
    "# if the gpu is available the model is moved on the gpu memory\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jJ0zoqiyzD3s"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def F1(output, target):\n",
    "  pred = torch.argmax(output, dim=1)\n",
    "  return f1_score(pred.cpu().detach().numpy(), target.cpu().detach().numpy(), average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "x-uvmh_5zLZL"
   },
   "outputs": [],
   "source": [
    "#pip install pytorchtools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "lD5tUOn2zOnB"
   },
   "outputs": [],
   "source": [
    "# create a database to store optuna studies with sqlite backend\n",
    "\n",
    "engine = create_engine('sqlite:///SA_optuna_tuning.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting botorch\n",
      "  Downloading botorch-0.4.0-py3-none-any.whl (395 kB)\n",
      "\u001b[K     |████████████████████████████████| 395 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.7.1 in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from botorch) (1.8.1)\n",
      "Requirement already satisfied: scipy in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from botorch) (1.5.2)\n",
      "Collecting gpytorch>=1.4\n",
      "  Downloading gpytorch-1.4.2-py2.py3-none-any.whl (492 kB)\n",
      "\u001b[K     |████████████████████████████████| 492 kB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from torch>=1.7.1->botorch) (1.19.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from torch>=1.7.1->botorch) (3.7.4.3)\n",
      "Requirement already satisfied: scikit-learn in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from gpytorch>=1.4->botorch) (0.24.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->gpytorch>=1.4->botorch) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->gpytorch>=1.4->botorch) (0.17.0)\n",
      "Installing collected packages: gpytorch, botorch\n",
      "Successfully installed botorch-0.4.0 gpytorch-1.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install botorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fgzeJth0nY0Y",
    "outputId": "e548228a-ec44-4135-acca-2abab2b752d2"
   },
   "outputs": [],
   "source": [
    "#!conda install botorch -c pytorch -c gpytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "NgNw-_5kzRoh"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import botorch\n",
    "from optuna.integration import BoTorchSampler\n",
    "\n",
    "class Param_Search():\n",
    "\n",
    "  def __init__(self, \n",
    "               #network_type,\n",
    "               train_loader, \n",
    "               test_loader,\n",
    "               criterion,\n",
    "               num_epochs,\n",
    "               study_name,\n",
    "               input_size,\n",
    "               sequence=False, \n",
    "               n_trials=4\n",
    "               ):\n",
    "    # self.network_type = network_type\n",
    "    self.train_loader = train_loader\n",
    "    self.test_loader = test_loader\n",
    "    self.criterion = criterion\n",
    "    self.num_epochs = num_epochs\n",
    "    self.study_name = study_name\n",
    "    self.input_size = input_size\n",
    "    self.sequence = sequence\n",
    "    self.n_trials = n_trials\n",
    "    self.best_model = None\n",
    "\n",
    "    self.oversample_SMOTE = SMOTE(k_neighbors=3)\n",
    "    self.oversample_SMOTEN = SMOTEN(k_neighbors=3)\n",
    "    self.onehot_encoder = OneHotEncoder(sparse=False).fit(np.array([0,1,2,4]).reshape(-1, 1)) \n",
    "    \n",
    "    \"\"\"Performs the hyper parameters tuning by using a TPE (Tree-structured Parzen Estimator) \n",
    "    algorithm sampler.  \n",
    "    \n",
    "    Parameters:\n",
    "    ------------------\n",
    "    model (torch.nn.Module): neural network model.\n",
    "    train_loader (DataLoader): training DataLoader object.\n",
    "    test_loader (DataLoader): testing DataLoader object.\n",
    "    criterion : loss function for training the model.\n",
    "    num_epochs (int): number of epochs.\n",
    "    study_name (str): name of the Optuna study object.\n",
    "    n_trial (int): number of trials to perform in the Optuna study.\n",
    "        Default: 4\n",
    "    \n",
    "    Attributes:\n",
    "    ------------------\n",
    "    best_model: stores the weights of the common layers of the best performing model.\n",
    "    \n",
    "    Returns:\n",
    "    ------------------\n",
    "    Prints values of the optimised hyperparameters and saves the parameters of the best model.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "  def objective(self, trial):\n",
    "    \"\"\"Defines the objective to be optimised (F1 test score) and saves\n",
    "    each final model.\n",
    "    \"\"\"\n",
    "\n",
    "    # generate the model\n",
    "    model = FFNN_define_model(trial, in_features_INPUT=self.input_size, classes=2)\n",
    "\n",
    "    # generate the possible optimizers\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\"])\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "\n",
    "    # convert model data type to double\n",
    "    model = model.double()\n",
    "\n",
    "    \n",
    "    # Define the training and testing phases\n",
    "    for epoch in tqdm(range(1, self.num_epochs + 1), desc='Epochs'):\n",
    "        train_loss = 0.0\n",
    "        test_loss = 0.0\n",
    "        f1_test = 0.0\n",
    "\n",
    "        # set the model in training modality\n",
    "        model.train()\n",
    "        for data, target in tqdm(self.train_loader, desc='Training Model'):\n",
    "            \n",
    "            if self.sequence:\n",
    "            #n_batches, n.elements per obs\n",
    "                data = data.reshape(data.shape[0], -1)\n",
    "                data,target = self.oversample_SMOTEN.fit_resample(data,target)\n",
    "                # one-hot encode (must do it after oversampling)\n",
    "                data_encoded = []\n",
    "                for x in data:\n",
    "                    data_encoded.append(self.onehot_encoder.transform(np.array(x).reshape(-1, 1)))\n",
    "                # final size: n_batches, n_channels, size of matrix (256*4)\n",
    "                data = torch.tensor(data_encoded).reshape(-1,1,256,4)\n",
    "\n",
    "            else:\n",
    "                data, target = self.oversample_SMOTE.fit_resample(data, target.ravel())\n",
    "                data = torch.tensor(data)\n",
    "        \n",
    "            target = torch.tensor(target)\n",
    "\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data.double())\n",
    "            # calculate the batch loss as a sum of the single losses\n",
    "            loss = self.criterion(output, target) \n",
    "            # backward pass: compute gradient of the loss wrt model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update training loss\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # set the model in testing modality\n",
    "        model.eval()\n",
    "        for data, target in tqdm(self.test_loader, desc='Testing Model'):  \n",
    "\n",
    "            if self.sequence:\n",
    "                # one-hot encode (must do it after oversampling)\n",
    "                data_encoded = []\n",
    "                for x in data:\n",
    "                    data_encoded.append(self.onehot_encoder.transform(x))\n",
    "                # final size: n_batches, n_channels, size of matrix (256*4)\n",
    "                data = torch.tensor(data_encoded).reshape(-1,1,256,4)\n",
    "        \n",
    "            target=target.reshape(-1) #########\n",
    "\n",
    "            # forward pass: compute predicted outputs by passing inputs to the model\n",
    "            output = model(data.double())\n",
    "            # calculate the batch loss as a sum of the single losses\n",
    "            loss = self.criterion(output, target)\n",
    "            # update test loss \n",
    "            test_loss += loss.item()\n",
    "            # calculate F1 test score as weighted sum of the single F1 scores\n",
    "            f1_test += F1(output,target)\n",
    "\n",
    "          # calculate epoch score by dividing by the number of observations\n",
    "        f1_test /= (len(self.test_loader))\n",
    "    \n",
    "        # pass the score of the epoch to the study to monitor the intermediate objective values\n",
    "        trial.report(f1_test, epoch)\n",
    "\n",
    "    # save the final model named with the number of the trial \n",
    "    with open(\"{}{}.pickle\".format(self.study_name, trial.number), \"wb\") as fout:\n",
    "        pickle.dump(model, fout)\n",
    "    \n",
    "    # return F1 score to the study\n",
    "    return f1_test\n",
    "\n",
    "\n",
    "\n",
    "  def run_trial(self):\n",
    "    \"\"\"Runs Optuna study and stores the best model in class attribute 'best_model'.\"\"\"\n",
    "    \n",
    "    # create a new study or load a pre-existing study. use sqlite backend to store the study.\n",
    "    study = optuna.create_study(study_name=self.study_name, direction=\"maximize\", \n",
    "                               # storage='sqlite:///SA_optuna_tuning.db', load_if_exists=True,\n",
    "                                sampler=BoTorchSampler())\n",
    "    \n",
    "    complete_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.COMPLETE]\n",
    "    pruned_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.PRUNED]\n",
    "    \n",
    "    # if the number of already completed trials is lower than the total number of trials passed as\n",
    "    #argument, perform the remaining trials \n",
    "    if len(complete_trials)<self.n_trials:\n",
    "        # set the number of trials to be performed equal to the number of missing trials\n",
    "        self.n_trials -= len(complete_trials)\n",
    "        study.optimize(self.objective, n_trials=self.n_trials)\n",
    "        pruned_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.PRUNED]\n",
    "        complete_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.COMPLETE]\n",
    "        \n",
    "    # store the best model found in the class\n",
    "    with open(\"{}{}.pickle\".format(self.study_name, study.best_trial.number), \"rb\") as fin:\n",
    "        best_model = pickle.load(fin)\n",
    "\n",
    "    self.best_model = best_model\n",
    "\n",
    "    \n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "\n",
    "    \n",
    "    \n",
    "  def save_best_model(self, path):\n",
    "    \"\"\"Saves the weights of the common layers of the best performing model.\n",
    "    \n",
    "    Parameters:\n",
    "    ------------------\n",
    "    path: path where the model will be stored.\n",
    "    \n",
    "    Returns:\n",
    "    ------------------\n",
    "    Weights of the common layers of the best model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # retrieve the weights of the best model\n",
    "    model_param = self.best_model.state_dict()\n",
    "    \n",
    "    # save only the weights of the common layers\n",
    "    for key,value in model_param.copy().items():\n",
    "        if re.findall('last', key):\n",
    "            del model_param[str(key)]\n",
    "\n",
    "    gdrive_path = '/content/gdrive/MyDrive/Thesis_BIOINF' ###\n",
    "    basepath = 'models' \n",
    "    basepath = grive_path + basepath ###\n",
    "    path = os.path.join(basepath, path)\n",
    "\n",
    "    torch.save(model_param, path)\n",
    "\n",
    "    return model_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "smPmNwJAzqrV"
   },
   "outputs": [],
   "source": [
    "# modified from https://github.com/Bjarten/early-stopping-pytorch/blob/master/pytorchtools.py\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    \n",
    "    Parameters:\n",
    "    ------------------\n",
    "        patience (int): How long to wait after last time validation loss improved.\n",
    "            Default: 7\n",
    "        verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "            Default: False\n",
    "        delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "            Default: 0\n",
    "        trace_func (function): trace print function.\n",
    "            Default: print \n",
    "                            \n",
    "    Attributes:\n",
    "    ------------------\n",
    "        early_stop (bool): True if the validation loss doesn't improveand the training should\n",
    "            be stopped, False else.\n",
    "        \"\"\"\n",
    "    \n",
    "    def __init__(self, patience=3, verbose=False, delta=0, trace_func=print):\n",
    "       \n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.delta = delta\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "        \n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        # if the new score is worse than the previous score, add 1 to the counter\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            # if the number of non-improving epochs is greater than patience, \n",
    "            #set to True early_stop attribute \n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "vPB5r6BSw49P"
   },
   "outputs": [],
   "source": [
    "import imblearn.over_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "id": "U69oq6Ikw0tX",
    "outputId": "b2f727c6-210f-44d8-f7e6-d0115b781e34"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "bJ2tyb5TzsSm"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "#from imblearn.over_sampling import SMOTEN\n",
    "    \n",
    "\n",
    "def fit(model, \n",
    "        train_loader, \n",
    "        test_loader, \n",
    "        criterion, \n",
    "        optimizer=None, \n",
    "        num_epochs=50, \n",
    "        pre_trained = False,\n",
    "        filename_path=None, \n",
    "        patience=3,\n",
    "        sequence=False,\n",
    "        delta=0,\n",
    "        verbose=True): \n",
    "    \n",
    "  \"\"\"Performs the training of the multitask model. It implements also early stopping\n",
    "    \n",
    "    Parameters:\n",
    "    ------------------\n",
    "    model (torch.nn.Module): neural network model.\n",
    "    train_loader (DataLoader): training DataLoader object.\n",
    "    test_loader (DataLoader): testing DataLoader object.\n",
    "    criterion: loss function for training the model.\n",
    "    optimizer (torch.optim): optimization algorithm for training the model. \n",
    "    num_epochs (int): number of epochs.\n",
    "    filename_path (str): where the weights of the model at each epoch will be stored. \n",
    "        Indicate only the name of the folder.\n",
    "    patience (int): number of epochs in which the test error is not anymore decreasing\n",
    "        before stopping the training.\n",
    "    delta (int): minimum decrease in the test error to continue with the training.\n",
    "        Default:0\n",
    "    verbose (bool): prints the training error, test error, F1 training score, F1 test score \n",
    "        at each epoch.\n",
    "        Default: True\n",
    "    \n",
    "    Attributes:\n",
    "    ------------------\n",
    "    f1_train_scores: stores the F1 training scores for each epoch.\n",
    "    f1_test_scores: stores the F1 test scores for each epoch.\n",
    "    \n",
    "    Returns:\n",
    "    ------------------\n",
    "    Lists of F1 training scores and F1 test scores at each epoch.\n",
    "    Prints training error, test error, F1 training score, F1 test score at each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "  gdrive_path = '/content/gdrive/MyDrive/Thesis_BIOINF' ###\n",
    "  basepath = 'exp'\n",
    "  basepath = gdrive_path + basepath ###\n",
    "\n",
    "  oversample_SMOTE = SMOTE(k_neighbors=3)\n",
    "  #oversample_SMOTEN = imblearn.over_sampling.SMOTEN(k_neighbors=3)\n",
    "  onehot_encoder = OneHotEncoder(sparse=False).fit(np.array([0,1,2,4]).reshape(-1, 1)) \n",
    "\n",
    "\n",
    "  # keep track of epoch losses \n",
    "  f1_train_scores = []\n",
    "  f1_test_scores = []\n",
    "\n",
    "  # convert model data type to double\n",
    "  model = model.double()\n",
    "\n",
    "  # define early stopping\n",
    "  early_stopping = EarlyStopping(patience=patience, delta=delta, verbose=True)\n",
    "    \n",
    "    \n",
    "  for epoch in tqdm(range(1, num_epochs + 1), desc='Epochs'):\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    f1_train = 0.0\n",
    "    f1_test = 0.0\n",
    "    \n",
    "    if pre_trained:\n",
    "        pass\n",
    "    \n",
    "    # if there is already a trained model stored for a specific epoch, load the model\n",
    "    #and don't retrain the model\n",
    "    elif os.path.exists( os.path.join(basepath, filename_path + '_' + str(epoch) + '.pt') ):\n",
    "      checkpoint = torch.load(PATH)\n",
    "      model.load_state_dict(checkpoint['model_state_dict'])\n",
    "      f1_train = checkpoint['F1_train']\n",
    "      f1_test = checkpoint['F1_test']\n",
    "      train_loss = checkpoint['train_loss']\n",
    "      test_loss = checkpoint['test_loss']\n",
    "    \n",
    "    else:\n",
    "\n",
    "      # set the model in training modality\n",
    "      model.train()\n",
    "\n",
    "      for data, target in tqdm(train_loader, desc='Training model'):\n",
    "        \n",
    "        # oversample using SMOTE / SMOTEN\n",
    "        if sequence:\n",
    "          #n_batches, n.elements per obs\n",
    "          data = data.reshape(data.shape[0], -1)\n",
    "          #data,target = oversample_SMOTEN.fit_resample(data,target)\n",
    "          # one-hot encode (must do it after oversampling)\n",
    "          data_encoded = []\n",
    "          for x in data:\n",
    "            data_encoded.append(onehot_encoder.transform(np.array(x).reshape(-1, 1)))\n",
    "          # final size: n_batches, n_channels, size of matrix (256*4)\n",
    "          data = torch.tensor(data_encoded).reshape(-1,1,256,4)\n",
    "\n",
    "        else:\n",
    "          data, target = oversample_SMOTE.fit_resample(data, target.ravel())\n",
    "          data = torch.tensor(data)\n",
    "        \n",
    "        target = torch.tensor(target)\n",
    "        \n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data.double())\n",
    "        # calculate the batch loss as the sum of all the losses\n",
    "        loss = criterion(output, target) \n",
    "        # backward pass: compute gradient of the loss wrt model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()\n",
    "        # calculate F1 training score as a weighted sum of the single F1 scores\n",
    "        f1_train += F1(output,target)\n",
    "\n",
    "        \n",
    "      # set the model in testing modality\n",
    "    model.eval()\n",
    "    for data, target in tqdm(test_loader, desc='Testing model'):\n",
    "\n",
    "        if sequence:\n",
    "          # one-hot encode (must do it after oversampling)\n",
    "          data_encoded = []\n",
    "          for x in data:\n",
    "            data_encoded.append(onehot_encoder.transform(x))\n",
    "          # final size: n_batches, n_channels, size of matrix (256*4)\n",
    "          data = torch.tensor(data_encoded).reshape(-1,1,256,4)\n",
    "        \n",
    "        target=target.reshape(-1)\n",
    "\n",
    "\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data.double())\n",
    "        # calculate the batch loss as the sum of all the losses\n",
    "        loss = criterion(output, target)\n",
    "        # update test loss\n",
    "        test_loss += loss.item()\n",
    "        # calculate F1 test score as a weighted sum of the single F1 scores\n",
    "        f1_test += F1(output,target) \n",
    "    \n",
    "    \n",
    "    if pre_trained:\n",
    "        continue\n",
    "    else:\n",
    "        # save the model weights, epoch, scores and losses at each epoch\n",
    "        model_param = model.state_dict()\n",
    "        PATH = os.path.join(basepath, filename_path + '_' + str(epoch) + '.pt')\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'model_state_dict': model_param,\n",
    "                    'F1_train': f1_train,\n",
    "                    'F1_test': f1_test,\n",
    "                    'train_loss': train_loss,\n",
    "                    'test_loss': test_loss},\n",
    "                   PATH)\n",
    "    \n",
    "    if pre_trained:\n",
    "        pass\n",
    "        # calculate epoch score by dividing by the number of observations\n",
    "        f1_train /= (len(train_loader))\n",
    "        f1_test /= (len(test_loader))\n",
    "    # store epoch score\n",
    "    f1_train_scores.append(f1_train)    \n",
    "    f1_test_scores.append(f1_test)\n",
    "      \n",
    "    # print training/test statistics \n",
    "    if verbose == True:\n",
    "      print('Epoch: {} \\tTraining F1 score: {:.4f} \\tTest F1 score: {:.4f} \\tTraining Loss: {:.4f} \\tTest Loss: {:.4f}'.format(\n",
    "      epoch, f1_train, f1_test, train_loss, test_loss))\n",
    "    \n",
    "    \n",
    "    if pre_trained:\n",
    "        pass\n",
    "    else:\n",
    "        # early stop the model if the test loss is not improving\n",
    "        early_stopping(test_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "          print('Early stopping the training')\n",
    "          # reload the previous best model before the test loss started decreasing\n",
    "          best_checkpoint = torch.load(os.path.join(basepath,filename_path + '_' + '{}'.format(epoch-patience) + '.pt'))\n",
    "          model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "          break\n",
    "\n",
    "  \n",
    "  # return the scores at each epoch\n",
    "  return f1_train_scores, f1_test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vaWtLyIs0D40"
   },
   "outputs": [],
   "source": [
    "def load_model(model, path):\n",
    "  \"\"\"Load the stored weights of a pre-trained model into another\n",
    "      model and set it to eval state.\n",
    "    \n",
    "    Parameters:\n",
    "    ------------------\n",
    "    model (torch.nn.Module): not trained neural network model.\n",
    "    path (str): path of the stored weights of the pre-trained model. \n",
    "    \"\"\"\n",
    "\n",
    "  gdrive_path = '/content/gdrive/MyDrive/Thesis_BIOINF' ###\n",
    "  basepath = 'models'\n",
    "  basepath = gdrive_path + basepath ###\n",
    "\n",
    "  path = os.path.join(basepath, path)\n",
    "  checkpoint = torch.load(path)\n",
    "  model.load_state_dict(checkpoint) \n",
    "  # set the model in testing modality\n",
    "  model.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ACCJJ5xX0E4a"
   },
   "outputs": [],
   "source": [
    "def save_best_model(model, path):\n",
    "    \"\"\"Saves only the weights of the common layers of a\n",
    "    trained neural network. \n",
    "    \n",
    "    Parameters:\n",
    "    ------------------\n",
    "    model (torch.nn.Module): trained neural network model.\n",
    "    path (str): path where the weights of the trained model will be stored. \n",
    "    \"\"\"\n",
    "    \n",
    "    model_param = model.state_dict()\n",
    "    for key,value in model_param.copy().items():\n",
    "      if re.findall('last', key):\n",
    "        del model_param[str(key)]\n",
    "\n",
    "    gdrive_path = '/content/gdrive/MyDrive/Thesis_BIOINF' ###\n",
    "    basepath = 'models'\n",
    "    basepath = gdrive_path + basepath ###\n",
    "    PATH = os.path.join(basepath, path)\n",
    "    \n",
    "    torch.save(model_param, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "yvhsw4Us0J_2"
   },
   "outputs": [],
   "source": [
    "def plot_model_scores(y_train, y_test, epochs, set_ylim=None):\n",
    "    \"\"\"Plots the trend of the training and test loss function of \n",
    "        a model.\n",
    "    \n",
    "    Parameters:\n",
    "    ------------------\n",
    "    y_train (list): list of training losses.\n",
    "    y_test (list): list of test losses.\n",
    "    epochs (int): number of epochs.\n",
    "    set_ylim (tuple of int): range of y-axis.\n",
    "        Default: None\n",
    "    \"\"\"\n",
    "   \n",
    "    epochs = range(epochs)\n",
    "    X=pd.DataFrame({'epochs':epochs,'y_train':y_train,'y_test':y_test})\n",
    "   \n",
    "    sns.set_theme(style=\"darkgrid\")\n",
    "    sns.set(rc={'figure.figsize':(30,15)})\n",
    "\n",
    "    f, ax = plt.subplots(1, 1)\n",
    "\n",
    "    sns.lineplot(data=X, x=\"epochs\", y=\"y_test\", color='red',lw=2.5)\n",
    "    sns.lineplot(data=X, x=\"epochs\", y=\"y_train\", color='green',lw=2.5)\n",
    "\n",
    "    plt.legend(labels=['F1 test score', 'F1 train score'])\n",
    "    plt.setp(ax.get_legend().get_texts(), fontsize=35)\n",
    "    plt.setp(ax.get_legend().get_title(),fontsize=35)\n",
    "\n",
    "    ax.set_ylabel('F1 score', fontsize=30)\n",
    "    ax.set_xlabel('Epochs', fontsize=30)\n",
    "    ax.tick_params(axis=\"y\", labelsize=20)\n",
    "    ax.tick_params(axis=\"x\", labelsize=20)\n",
    "    ax.set_ylim(set_ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "PDshoQQHHQ2t"
   },
   "outputs": [],
   "source": [
    "def accuracy(y_hat, y):\n",
    "  pred = torch.argmax(y_hat, dim=1)\n",
    "  # return the category with the highest probability\n",
    "  return (pred == y).float().mean()\n",
    "  # return true if the predicted category is equal to the true one, false otherwise.\n",
    "  #we transform them in float, 1 if True, 0 is False, then we return the mean of the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2IoSXUq1afQ"
   },
   "source": [
    "# 1. FEED FORWARD NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "QgZRa7ixiqkh"
   },
   "outputs": [],
   "source": [
    "def FFNN_define_model(trial, in_features_INPUT, classes=2):\n",
    "    # We optimize the number of layers, hidden units and dropout ratio in each layer.\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    layers = []\n",
    "\n",
    "    in_features = in_features_INPUT\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 4, in_features_INPUT)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        p = trial.suggest_float(\"dropout_l{}\".format(i), 0.2, 0.5)\n",
    "        layers.append(nn.Dropout(p))\n",
    "\n",
    "        in_features = out_features\n",
    "\n",
    "    layers.append(nn.Linear(in_features, classes))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "LRJuJEnK1mit"
   },
   "outputs": [],
   "source": [
    "class FFNN(nn.Module):\n",
    "  \"\"\" Feed Forward neural network. It uses ReLU activation functions.\"\"\"\n",
    "\n",
    "  def __init__(self, input_size):\n",
    "    super(FFNN, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    \n",
    "    self.layer1 = nn.Sequential(\n",
    "        nn.Linear(self.input_size, 100), \n",
    "        nn.ReLU())\n",
    "    self.layer2 = nn.Sequential(\n",
    "        nn.Linear(100, 50),\n",
    "        nn.ReLU()) \n",
    "    \n",
    "\n",
    "    self.last_layer = nn.Linear(50, 2) \n",
    "    # mat1 and mat2 shapes cannot be multiplied (190x50 and 540x100)\n",
    "\n",
    "    self.drop_out1 = nn.Dropout(p=0.3)\n",
    "    self.drop_out2 = nn.Dropout(p=0.4) \n",
    "\n",
    "  def forward(self, x):\n",
    "      \n",
    "      \n",
    "    out = self.layer1(x)\n",
    "    out = self.drop_out1(out)\n",
    "\n",
    "    out = self.layer2(out)\n",
    "    out = self.drop_out2(out)\n",
    "    out = self.last_layer(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0iVxIZcMw_s"
   },
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nItYRVDo10uR"
   },
   "source": [
    "## Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "J6reuM4o1gjW"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "_-SFzs7q1A7E"
   },
   "outputs": [],
   "source": [
    "train_loader, test_loader = pipe_data_load.return_data(cell_line='HEPG2', \n",
    "                    hyper_tuning=True, \n",
    "                    sequence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ZeskXVx61FF6"
   },
   "outputs": [],
   "source": [
    "def get_input_size_FFNN(data_loader):\n",
    "  for d,l in data_loader:\n",
    "    input_size = d.shape[1]\n",
    "    break\n",
    "  return input_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "OyoPA6SI3HTs"
   },
   "outputs": [],
   "source": [
    "input_size = get_input_size_FFNN(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "6AQS6rqr18O7"
   },
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 647
    },
    "id": "d-gQioBg19K9",
    "outputId": "bf4f61d5-3cb7-42f7-b407-cdd39f61aac8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-68-85cc99011468>:158: ExperimentalWarning: BoTorchSampler is experimental (supported from v2.4.0). The interface can change in the future.\n",
      "  sampler=BoTorchSampler())\n",
      "\u001b[32m[I 2021-06-14 14:09:42,313]\u001b[0m A new study created in memory with name: hp_FFNN\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f675cec21240d08b302d84fc29f058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epochs'), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e0556606904dd6ba1bed1ea66480e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training Model'), FloatProgress(value=0.0, max=404.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "babeac58b7e24f19909830c71e5a904a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Testing Model'), FloatProgress(value=0.0, max=36.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82eae5c81bf440e89ed9182394d081e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training Model'), FloatProgress(value=0.0, max=404.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8688920432d40fab826dcaf969d9efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Testing Model'), FloatProgress(value=0.0, max=36.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-06-14 14:14:36,524]\u001b[0m Trial 0 finished with value: 0.20761762003837356 and parameters: {'n_layers': 3, 'n_units_l0': 369, 'dropout_l0': 0.4357738320752208, 'n_units_l1': 455, 'dropout_l1': 0.4073078141474769, 'n_units_l2': 347, 'dropout_l2': 0.4731143539113111, 'optimizer': 'RMSprop', 'lr': 0.04321534024331291}. Best is trial 0 with value: 0.20761762003837356.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228fd234b32e4c24b777b447546fb9e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epochs'), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e8fc93919949bc87717b5a82674b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training Model'), FloatProgress(value=0.0, max=404.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23dd7e0c7dc64e319ff02997f3958189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Testing Model'), FloatProgress(value=0.0, max=36.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dad2c92484c4200a08cf6609fa59211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training Model'), FloatProgress(value=0.0, max=404.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2f18690e2d4ed4b3a964ba4b15f7ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Testing Model'), FloatProgress(value=0.0, max=36.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-06-14 14:18:30,514]\u001b[0m Trial 1 finished with value: 0.8307348694286952 and parameters: {'n_layers': 2, 'n_units_l0': 216, 'dropout_l0': 0.3155312391750696, 'n_units_l1': 459, 'dropout_l1': 0.28529844778180163, 'optimizer': 'RMSprop', 'lr': 2.7520615182966584e-05}. Best is trial 1 with value: 0.8307348694286952.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Study statistics: \n",
      "  Number of finished trials:  2\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  2\n",
      "Best trial:\n",
      "  Value:  0.8307348694286952\n",
      "  Params: \n",
      "    n_layers: 2\n",
      "    n_units_l0: 216\n",
      "    dropout_l0: 0.3155312391750696\n",
      "    n_units_l1: 459\n",
      "    dropout_l1: 0.28529844778180163\n",
      "    optimizer: RMSprop\n",
      "    lr: 2.7520615182966584e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Niki/opt/anaconda3/lib/python3.8/site-packages/optuna/structs.py:18: FutureWarning: `structs` is deprecated. Classes have moved to the following modules. `structs.StudyDirection`->`study.StudyDirection`, `structs.StudySummary`->`study.StudySummary`, `structs.FrozenTrial`->`trial.FrozenTrial`, `structs.TrialState`->`trial.TrialState`, `structs.TrialPruned`->`exceptions.TrialPruned`.\n",
      "  warnings.warn(_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "param_search = Param_Search(train_loader, test_loader,\n",
    "            criterion, num_epochs, input_size = input_size, sequence=False,\n",
    "            n_trials=2, study_name='hp_FFNN')\n",
    "\n",
    "param_search.run_trial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPq8ax182E1R"
   },
   "outputs": [],
   "source": [
    "best_model_FFNN_hp = param_search.save_best_model('FFNN/best_model_FFNN_hp.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpe-USry2QQv"
   },
   "source": [
    "## Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "1M3XcKM10qMv"
   },
   "outputs": [],
   "source": [
    "#train_loader, test_loader = pipe_data_load.return_data(cell_line='H1', \n",
    " #                   hyper_tuning=True, \n",
    "  #                  sequence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m4K2tLPT2SU0",
    "outputId": "35687646-db35-491a-e4cb-811cefc2f6f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FFNN(\n",
       "  (layer1): Sequential(\n",
       "    (0): Linear(in_features=540, out_features=100, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Linear(in_features=100, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (last_layer): Linear(in_features=50, out_features=2, bias=True)\n",
       "  (drop_out1): Dropout(p=0.3, inplace=False)\n",
       "  (drop_out2): Dropout(p=0.4, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=FFNN(input_size=input_size)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "TBnanKg92jG0"
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "IqAKKNJu2kAp"
   },
   "outputs": [],
   "source": [
    "best_lr = 3.0174993222703274e-05 ##TRAIN\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "a6d85f1038cd41da900f5edc0e80811d",
      "35b5608b0dc44af080ce58a099cd8712",
      "8fb7a0d167284bf79ff068a5aa85e9d2",
      "87899c5edd9342d2ac4b4ccc85ffc239",
      "61e41656f25e4a31b2fa8b673df87c31",
      "dc5de6784a954a5987ec7b2e85238215",
      "1726012577934d22976d0d887c1e9550",
      "1e7aad6fee9c4a78a819560cf7df65d1",
      "f1f8579b4d1b47b08269c9a8fbea17be",
      "f589903ecd1448d6a0eab7fce3b822f2",
      "6172d0db84ac405d90a81ca17e031d5e",
      "0bb7a6aef6f344068b1a1ef798debead",
      "52224afa7ed04cd59f64a33a124f15f8",
      "51871383359e47c5be7e7ed3763f8024",
      "0266ac46c9b9453a9539ea39e6710364",
      "b6812b69f4f840f0b2f733b077e83919",
      "1b704b6614534e2286d7203527c1c216",
      "a68fcc18c0f64e0f9bf6d3002bcab74d",
      "a5f107767969497fb1980cc0ab6262b0",
      "7b802877c3dc4c759de9a0bfc10ff400",
      "131a0ead97934369875db3b372dc2a2a",
      "316a7ea0a7af45a18856c3e17d3f8565",
      "8a4f2ba83b2a4291a5b332d025200abb",
      "2274bbb4d74148b2bf414c324b406965"
     ]
    },
    "id": "YvUj5GJs21OR",
    "outputId": "4edc335c-8663-47dc-a7d6-c135f3808212"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3b43288209844b98af0e967f0b70aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epochs'), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e20dc7c3024b3fbb2e3fce3abda43e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Training model'), FloatProgress(value=0.0, max=404.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-4599b6d65619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m F1_train, F1_test = fit(model, train_loader, test_loader, criterion, optimizer, \n\u001b[0m\u001b[1;32m      2\u001b[0m                         \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'FFNN/ffnn_testing'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         sequence=False, verbose=True)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#save_best_model(model, 'FFNN/best_model_FFNN_test.pt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-4032eb2b8cfb>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, train_loader, test_loader, criterion, optimizer, num_epochs, filename_path, patience, sequence, delta, verbose)\u001b[0m\n\u001b[1;32m     90\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;31m# oversample using SMOTE / SMOTEN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1165\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1166\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-261e23ef6093>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1501\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1503\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ixs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   2949\u001b[0m             \u001b[0;31m# if we are a copy, mark as such\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2950\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2951\u001b[0;31m             result = self._constructor_sliced(\n\u001b[0m\u001b[1;32m   2952\u001b[0m                 \u001b[0mnew_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2953\u001b[0m                 \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    362\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    543\u001b[0m                 \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m         \u001b[0mis_object_or_str_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_string_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_object_or_str_dtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0minferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_string_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_excluded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mis_excluded\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mis_excluded_checks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_is_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36m_is_dtype\u001b[0;34m(arr_or_dtype, condition)\u001b[0m\n\u001b[1;32m   1570\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnicodeEncodeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1571\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1572\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mcondition\u001b[0;34m(dtype)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \"\"\"\n\u001b[1;32m    601\u001b[0m     \u001b[0;31m# TODO: gh-15585: consider making the checks stricter.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"O\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"S\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"U\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_excluded_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "F1_train, F1_test = fit(model, train_loader, test_loader, criterion, optimizer, \n",
    "                        num_epochs, pre_trained=True, filename_path='FFNN/ffnn_testing', patience=3,\n",
    "                        sequence=True, verbose=True)\n",
    "\n",
    "#save_best_model(model, 'FFNN/best_model_FFNN_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6q7ojMcD231j"
   },
   "outputs": [],
   "source": [
    "plot_model_scores(F1_train, F1_test, epochs=20,set_ylim=(0.82,0.88))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iu4BLdy81hvH"
   },
   "source": [
    "# 2. CONVOLUTIONAL NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_define_model(trial, in_features_INPUT, classes=2):\n",
    "    # We optimize the number of layers, hidden units and dropout ratio in each layer.\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    layers = []\n",
    "\n",
    "    in_channels = 1\n",
    "    for i in range(n_layers):\n",
    "        out_channels = trial.suggest_int(\"out_channels_l{}\".format(i), 16, 128)\n",
    "#        kernel_size = trial.suggest_int(\"kernel_size_l{}\".format(i), 3, 15)\n",
    "        layers.append( nn.Conv1d(in_channels, out_channels, kernel_size=5, stride=1, padding=1) )\n",
    "        layers.append( nn.BatchNorm1d(out_channels) )\n",
    "        layers.append( nn.ReLU() )\n",
    "        layers.append( nn.MaxPool1d(kernel_size=3, stride=1) )\n",
    "\n",
    "        p = trial.suggest_float(\"dropout_l{}\".format(i), 0.2, 0.5)\n",
    "        layers.append(nn.Dropout(p))\n",
    "\n",
    "        in_channels = out_channels\n",
    "\n",
    "    out = out.reshape(out.size(0), -1) \n",
    "    self.last_layer1 = nn.Linear(self.fc_layer_size, 1000) \n",
    "    self.last_layer2 = nn.Linear(1000, self.classes)\n",
    "    layers.append(nn.Linear(in_features, classes))\n",
    "\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FeQBrIDh1qti"
   },
   "outputs": [],
   "source": [
    "def __init__(self, fc_layer_size, classes=2):\n",
    "    super(CNN_multitask, self).__init__()\n",
    "    self.fc_layer_size = fc_layer_size \n",
    "    self.classes = classes\n",
    "    \n",
    "    self.layer1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 32, kernel_size=5, stride=1, padding=1), #The average word length in English language is 4.7 characters.\n",
    "            nn.BatchNorm1d(32), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1))\n",
    "\n",
    "    self.layer2 = nn.Sequential(\n",
    "            nn.Conv1d(32, 32, kernel_size=5, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1))\n",
    "    \n",
    "    self.layer3 = nn.Sequential(\n",
    "            nn.Conv1d(32, 64, kernel_size=5, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=1)\n",
    "    \n",
    "        \n",
    "    self.drop_out1 = nn.Dropout(p=0.3)\n",
    "    self.drop_out2 = nn.Dropout(p=0.4)\n",
    "    self.drop_out3 = nn.Dropout(p=0.5)\n",
    "    \n",
    "    self.last_layer1 = nn.Linear(self.fc_layer_size, 1000) \n",
    "    self.last_layer2 = nn.Linear(1000, self.classes)\n",
    "    \n",
    "\n",
    "  def forward(self, x):\n",
    "      \n",
    "      # 2 shared blocks \n",
    "    out = self.layer1(x)\n",
    "    out = self.drop_out1(out)\n",
    "    out = self.layer2(out)\n",
    "    out = self.drop_out2(out)\n",
    "    out = self.layer3(out) \n",
    "    out = self.drop_out3(out)\n",
    "    \n",
    "    out = out.reshape(out.size(0), -1) \n",
    "    out = self.last_layer1(out)\n",
    "    out = self.last_layer2(out)\n",
    "\n",
    " \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet_pytorch\n",
      "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
      "Requirement already satisfied: torch in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from efficientnet_pytorch) (1.8.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from torch->efficientnet_pytorch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /Users/Niki/opt/anaconda3/lib/python3.8/site-packages (from torch->efficientnet_pytorch) (1.19.2)\n",
      "Building wheels for collected packages: efficientnet-pytorch\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16445 sha256=096e4125e90c900940912447fa9b888cab206e9b03a44ef24a873ffb7b0a33f4\n",
      "  Stored in directory: /Users/Niki/Library/Caches/pip/wheels/84/b9/90/25a0195cf95fb5533db96f1c77ea3f296b7cc86ae8ae48e3dc\n",
      "Successfully built efficientnet-pytorch\n",
      "Installing collected packages: efficientnet-pytorch\n",
      "Successfully installed efficientnet-pytorch-0.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install efficientnet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b7\n"
     ]
    }
   ],
   "source": [
    "from efficientnet_pytorch import EfficientNet\n",
    "model = EfficientNet.from_pretrained('efficientnet-b7', num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (_conv_stem): Conv2dStaticSamePadding(\n",
       "    3, 64, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "  )\n",
       "  (_bn0): BatchNorm2d(64, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (_blocks): ModuleList(\n",
       "    (0): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        64, 64, kernel_size=(3, 3), stride=[1, 1], groups=64, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(64, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        64, 16, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        16, 64, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (1): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (2): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (3): MBConvBlock(\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(3, 3), stride=(1, 1), groups=32, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (4): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        192, 192, kernel_size=(3, 3), stride=[2, 2], groups=192, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        192, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        8, 192, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (5): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        288, 12, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        12, 288, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (6): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        288, 12, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        12, 288, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (7): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        288, 12, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        12, 288, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (8): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        288, 12, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        12, 288, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (9): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        288, 12, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        12, 288, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (10): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        288, 288, kernel_size=(3, 3), stride=(1, 1), groups=288, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        288, 12, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        12, 288, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(48, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (11): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        288, 288, kernel_size=(5, 5), stride=[2, 2], groups=288, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(288, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        288, 12, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        12, 288, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        288, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (12): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (13): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (14): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (15): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (16): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (17): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(5, 5), stride=(1, 1), groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (18): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        480, 480, kernel_size=(3, 3), stride=[2, 2], groups=480, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        480, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (19): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (20): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (21): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (22): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (23): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (24): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (25): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (26): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (27): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(3, 3), stride=(1, 1), groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(160, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (28): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        960, 960, kernel_size=(5, 5), stride=[1, 1], groups=960, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(960, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        960, 40, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        40, 960, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        960, 224, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (29): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1344, 56, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        56, 1344, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (30): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1344, 56, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        56, 1344, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (31): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1344, 56, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        56, 1344, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (32): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1344, 56, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        56, 1344, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (33): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1344, 56, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        56, 1344, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (34): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1344, 56, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        56, 1344, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (35): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1344, 56, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        56, 1344, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (36): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1344, 56, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        56, 1344, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (37): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1344, 1344, kernel_size=(5, 5), stride=(1, 1), groups=1344, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1344, 56, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        56, 1344, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1344, 224, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(224, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (38): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        224, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        1344, 1344, kernel_size=(5, 5), stride=[2, 2], groups=1344, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(1344, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        1344, 56, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        56, 1344, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        1344, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (39): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        2304, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        96, 2304, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (40): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        2304, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        96, 2304, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (41): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        2304, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        96, 2304, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (42): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        2304, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        96, 2304, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (43): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        2304, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        96, 2304, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (44): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        2304, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        96, 2304, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (45): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        2304, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        96, 2304, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (46): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        2304, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        96, 2304, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (47): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        2304, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        96, 2304, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (48): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        2304, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        96, 2304, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (49): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        2304, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        96, 2304, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (50): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        2304, 2304, kernel_size=(5, 5), stride=(1, 1), groups=2304, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        2304, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        96, 2304, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        2304, 384, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(384, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (51): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        384, 2304, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        2304, 2304, kernel_size=(3, 3), stride=[1, 1], groups=2304, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(2304, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        2304, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        96, 2304, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        2304, 640, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(640, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (52): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        3840, 3840, kernel_size=(3, 3), stride=(1, 1), groups=3840, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        3840, 160, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        160, 3840, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(640, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (53): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        3840, 3840, kernel_size=(3, 3), stride=(1, 1), groups=3840, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        3840, 160, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        160, 3840, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(640, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "    (54): MBConvBlock(\n",
       "      (_expand_conv): Conv2dStaticSamePadding(\n",
       "        640, 3840, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn0): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "        3840, 3840, kernel_size=(3, 3), stride=(1, 1), groups=3840, bias=False\n",
       "        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "      )\n",
       "      (_bn1): BatchNorm2d(3840, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_se_reduce): Conv2dStaticSamePadding(\n",
       "        3840, 160, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_se_expand): Conv2dStaticSamePadding(\n",
       "        160, 3840, kernel_size=(1, 1), stride=(1, 1)\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_project_conv): Conv2dStaticSamePadding(\n",
       "        3840, 640, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "        (static_padding): Identity()\n",
       "      )\n",
       "      (_bn2): BatchNorm2d(640, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "      (_swish): MemoryEfficientSwish()\n",
       "    )\n",
       "  )\n",
       "  (_conv_head): Conv2dStaticSamePadding(\n",
       "    640, 2560, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "    (static_padding): Identity()\n",
       "  )\n",
       "  (_bn1): BatchNorm2d(2560, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "  (_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (_fc): Linear(in_features=2560, out_features=2, bias=True)\n",
       "  (_swish): MemoryEfficientSwish()\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = pipe_data_load.return_data(cell_line='H1', \n",
    "                    hyper_tuning=True, \n",
    "                    sequence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab087b78ebd4bfb9c369b66127b772c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epochs'), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c6cb92d0414c1b9e44cd7720f3739e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Testing model'), FloatProgress(value=0.0, max=36.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 3, 3], expected input[192, 1, 257, 5] to have 3 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-0ffd8b036a7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m F1_train, F1_test = fit(model, train_loader, test_loader, criterion, optimizer, \n\u001b[0m\u001b[1;32m      2\u001b[0m                         \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_trained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'FFNN/ffnn_testing'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         sequence=True, verbose=True)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#save_best_model(model, 'FFNN/best_model_FFNN_test.pt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-98-202667f61e6c>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(model, train_loader, test_loader, criterion, optimizer, num_epochs, pre_trained, filename_path, patience, sequence, delta, verbose)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# forward pass: compute predicted outputs by passing inputs to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0;31m# calculate the batch loss as the sum of all the losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/efficientnet_pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \"\"\"\n\u001b[1;32m    313\u001b[0m         \u001b[0;31m# Convolution layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;31m# Pooling and final linear layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_avg_pooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/efficientnet_pytorch/model.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \"\"\"\n\u001b[1;32m    288\u001b[0m         \u001b[0;31m# Stem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bn0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_stem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;31m# Blocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/efficientnet_pytorch/utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 3, 3], expected input[192, 1, 257, 5] to have 3 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "F1_train, F1_test = fit(model, train_loader, test_loader, criterion, optimizer, \n",
    "                        num_epochs, pre_trained=True, filename_path='FFNN/ffnn_testing', patience=3,\n",
    "                        sequence=True, verbose=True)\n",
    "\n",
    "#save_best_model(model, 'FFNN/best_model_FFNN_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzYWmpjmYPIm"
   },
   "source": [
    "# EMBRACENET APPLICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BH_kpTB8CtJC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class EmbraceNet(nn.Module):\n",
    "  \n",
    "  def __init__(self, device, input_size_list, embracement_size=256, bypass_docking=False):\n",
    "    \"\"\"\n",
    "    Initialize an EmbraceNet module.\n",
    "    Args:\n",
    "      device: A \"torch.device()\" object to allocate internal parameters of the EmbraceNet module.\n",
    "      input_size_list: A list of input sizes.\n",
    "      embracement_size: The length of the output of the embracement layer (\"c\" in the paper).\n",
    "      bypass_docking: Bypass docking step, i.e., connect the input data directly to the embracement layer. If True, input_data must have a shape of [batch_size, embracement_size].\n",
    "    \"\"\"\n",
    "    super(EmbraceNet, self).__init__()\n",
    "\n",
    "    self.device = device\n",
    "    self.input_size_list = input_size_list\n",
    "    self.embracement_size = embracement_size\n",
    "    self.bypass_docking = bypass_docking\n",
    "\n",
    "    if (not bypass_docking):\n",
    "      for i, input_size in enumerate(input_size_list):\n",
    "        setattr(self, 'docking_%d' % (i), nn.Linear(input_size, embracement_size))\n",
    "\n",
    "\n",
    "  def forward(self, input_list, availabilities=None, selection_probabilities=None):\n",
    "    \"\"\"\n",
    "    Forward input data to the EmbraceNet module.\n",
    "    Args:\n",
    "      input_list: A list of input data. Each input data should have a size as in input_size_list.\n",
    "      availabilities: A 2-D tensor of shape [batch_size, num_modalities], which represents the availability of data for each modality. If None, it assumes that data of all modalities are available.\n",
    "      selection_probabilities: A 2-D tensor of shape [batch_size, num_modalities], which represents probabilities that output of each docking layer will be selected (\"p\" in the paper). If None, the same probability of being selected will be used for each docking layer.\n",
    "    Returns:\n",
    "      A 2-D tensor of shape [batch_size, embracement_size] that is the embraced output.\n",
    "    \"\"\"\n",
    "\n",
    "    # check input data\n",
    "    assert len(input_list) == len(self.input_size_list)\n",
    "    num_modalities = len(input_list)\n",
    "    batch_size = input_list[0].shape[0]\n",
    "    \n",
    "\n",
    "    # docking layer\n",
    "    docking_output_list = []\n",
    "    if (self.bypass_docking):\n",
    "      docking_output_list = input_list\n",
    "    else:\n",
    "      for i, input_data in enumerate(input_list):\n",
    "        x = getattr(self, 'docking_%d' % (i))(input_data)\n",
    "        x = nn.functional.relu(x)\n",
    "        docking_output_list.append(x)\n",
    "    \n",
    "\n",
    "    # check availabilities\n",
    "    if (availabilities is None):\n",
    "      availabilities = torch.ones(batch_size, len(input_list), dtype=torch.float, device=self.device)\n",
    "    else:\n",
    "      availabilities = availabilities.float()\n",
    "    \n",
    "\n",
    "    # adjust selection probabilities\n",
    "    if (selection_probabilities is None):\n",
    "      selection_probabilities = torch.ones(batch_size, len(input_list), dtype=torch.float, device=self.device)\n",
    "    selection_probabilities = torch.mul(selection_probabilities, availabilities)\n",
    "\n",
    "    probability_sum = torch.sum(selection_probabilities, dim=-1, keepdim=True)\n",
    "    selection_probabilities = torch.div(selection_probabilities, probability_sum)\n",
    "\n",
    "\n",
    "    # stack docking outputs\n",
    "    docking_output_stack = torch.stack(docking_output_list, dim=-1)  # [batch_size, embracement_size, num_modalities]\n",
    "\n",
    "\n",
    "    # embrace\n",
    "    modality_indices = torch.multinomial(selection_probabilities, num_samples=self.embracement_size, replacement=True)  # [batch_size, embracement_size]\n",
    "    modality_toggles = nn.functional.one_hot(modality_indices, num_classes=num_modalities).float()  # [batch_size, embracement_size, num_modalities]\n",
    "\n",
    "    embracement_output_stack = torch.mul(docking_output_stack, modality_toggles)\n",
    "    embracement_output = torch.sum(embracement_output_stack, dim=-1)  # [batch_size, embracement_size]\n",
    "\n",
    "    return embracement_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "goeL7ZZU--j-"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class VGG_pre(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG_pre, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32), \n",
    "            nn.ReLU(),\n",
    "      #      nn.Conv2d(32, 32, kernel_size=5, stride=1, padding=2),\n",
    "       #     nn.BatchNorm2d(32),\n",
    "        #    nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "     #       nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=2),\n",
    "      #      nn.BatchNorm2d(64),\n",
    "       #     nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.drop_out1 = nn.Dropout(p=0.3)\n",
    "        self.drop_out2 = nn.Dropout(p=0.4)\n",
    "       \n",
    "\n",
    "    def forward(self, x):\n",
    "      out = self.layer1(x)\n",
    "      out = self.drop_out1(out)\n",
    "      out = self.layer2(out)\n",
    "      out = self.drop_out2(out)\n",
    "      out = out.reshape(out.size(0), -1) \n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEBTsRqCbZLh"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "training_loader = defaultdict(lambda:0)\n",
    "validation_loader = defaultdict(lambda:0)\n",
    "\n",
    "training_loader['left'] = loader_imag_train_L\n",
    "training_loader['central'] = loader_imag_train_C\n",
    "training_loader['right'] = loader_imag_train_R\n",
    "\n",
    "validation_loader['left'] = loader_imag_validation_L\n",
    "validation_loader['central'] = loader_imag_validation_C\n",
    "validation_loader['right'] = loader_imag_validation_R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cenBsd-Xyxhy",
    "outputId": "2fbc2005-7fcb-4ca3-ea14-96b9a571cace"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>>,\n",
       "            {'central': <torch.utils.data.dataloader.DataLoader at 0x7f0810cb6e10>,\n",
       "             'left': <torch.utils.data.dataloader.DataLoader at 0x7f07ffb169d0>,\n",
       "             'right': <torch.utils.data.dataloader.DataLoader at 0x7f07ffb92250>})"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oF9zMxSKYN1n"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class EmbraceNetMultimodal(nn.Module):\n",
    "  def __init__(self, device, n_classes, hyperparameters_tuning=False, args=None):\n",
    "    super(EmbraceNetMultimodal, self).__init__()\n",
    "    \n",
    "    \n",
    "    # input parameters\n",
    "    self.device = device\n",
    "    self.n_classes = n_classes\n",
    "    self.args = args\n",
    "    self.hyperparameters_tuning = hyperparameters_tuning\n",
    "\n",
    "    \n",
    "    # VGG convolutional neural network\n",
    "    self.VGG_left = VGG_pre()\n",
    "    self.VGG_central = VGG_pre()\n",
    "    self.VGG_right = VGG_pre()\n",
    "\n",
    "    # load previously trained models to find optimal hyperparameters\n",
    "    if self.hyperparameters_tuning:\n",
    "      load_model(self.VGG_left, 'best_model_L_hp.pt')\n",
    "      load_model(self.VGG_central, 'best_model_C_hp.pt')\n",
    "      load_model(self.VGG_right, 'best_model_R_hp.pt')\n",
    "    \n",
    "    # load previously trained models for final testing\n",
    "    else:\n",
    "      load_model(self.VGG_left, 'best_model_L_test.pt')\n",
    "      load_model(self.VGG_central, 'best_model_C_test.pt')\n",
    "      load_model(self.VGG_right, 'best_model_R_test.pt')\n",
    "\n",
    "    # freeze layers\n",
    "    for param in self.VGG_left.parameters():\n",
    "      param.requires_grad = False\n",
    "    for param in self.VGG_central.parameters():\n",
    "      param.requires_grad = False\n",
    "    for param in self.VGG_right.parameters():\n",
    "      param.requires_grad = False\n",
    "        \n",
    "    self.pre_output_size = (5*64*16) #5120\n",
    "\n",
    "    # embracenet\n",
    "    self.embracenet = EmbraceNet(device=self.device, \n",
    "                                 input_size_list=[self.pre_output_size, self.pre_output_size, self.pre_output_size], \n",
    "                                 embracement_size=512)\n",
    "\n",
    "    # post embracement layers\n",
    "    self.post = nn.Linear(in_features=512, out_features=self.n_classes)\n",
    "\n",
    "  \n",
    "  def forward(self, x, availabilities=None, selection_probabilities=None):\n",
    "\n",
    "    x_left, x_central, x_right = x\n",
    "\n",
    "    x_left_final = self.VGG_left(x_left)\n",
    "    x_central_final = self.VGG_central(x_central)\n",
    "    x_right_final = self.VGG_right(x_right)\n",
    "\n",
    "    # drop left or right modality\n",
    "   # availabilities = None\n",
    "    #if (self.args.model_drop_left or self.args.model_drop_central or self.args.model_drop_right):\n",
    "   #   availabilities = torch.ones([x.shape[0], 3], device=self.device)\n",
    "    #  if (self.args.model_drop_left):\n",
    "     #   availabilities[:, 0] = 0\n",
    "   #   if (self.args.model_drop_central):\n",
    "    #    availabilities[:, 1] = 0\n",
    "     # if (self.args.model_drop_right):\n",
    "      #  availabilities[:, 2] = 0\n",
    "\n",
    "    # dropout during training\n",
    " #   if (self.is_training and self.args.model_dropout):\n",
    "  #    dropout_prob = torch.rand(1, device=self.device)[0]\n",
    "   #   if (dropout_prob >= 0.5):\n",
    "    #    target_modalities = torch.round(torch.rand([x.shape[0]], device=self.device)).to(torch.int64)\n",
    "     #  availabilities = nn.functional.one_hot(target_modalities, num_classes=2).float()\n",
    "\n",
    "    # embrace\n",
    "    embracenet = self.embracenet([x_left_final, x_central_final, x_right_final]) #, availabilities=availabilities)\n",
    "\n",
    "    # employ final layers\n",
    "    output = self.post(embracenet)\n",
    "\n",
    "    # output softmax\n",
    "    return output\n",
    "    \n",
    "   # return nn.functional.log_softmax(output, dim=-1) #not needed since it's already applied\n",
    "   #by cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6Szz7dYTiYZ"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 50\n",
    "num_classes = 5\n",
    "#learning_rate=0.0001\n",
    "#optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate)\n",
    "\n",
    "#optimizer = torch.optim.Adam([x_left, x_central, x_right],lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "atm7JYZTRRWH"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch.nn as nn\n",
    "#import thop\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "class Param_Search_Multimodal():\n",
    "\n",
    "  \"\"\"Performs the hyper parameters tuning by using a TPE (Tree-structured Parzen Estimator) \n",
    "    algorithm sampler.  \n",
    "    \n",
    "    Parameters:\n",
    "    ------------------\n",
    "    model (torch.nn.Module): neural network model.\n",
    "    train_loader (DataLoader): dictionary of training DataLoader objects. Keys of the\n",
    "        dictionary must be 'FFNN', 'CNN', 'D2V_CNN'.\n",
    "    test_loader (DataLoader): dictionary of testing DataLoader objects. Keys of the\n",
    "        dictionary must be 'FFNN', 'CNN', 'D2V_CNN'.\n",
    "    criterion : loss function for training the model.\n",
    "    num_epochs (int): number of epochs.\n",
    "    study_name (str): name of the Optuna study object.\n",
    "    n_trial (int): number of trials to perform in the Optuna study.\n",
    "        Default: 4\n",
    "    \n",
    "    Attributes:\n",
    "    ------------------\n",
    "    best_model: stores the weights of the common layers of the best performing model.\n",
    "    \n",
    "    Returns:\n",
    "    ------------------\n",
    "  Prints values of the optimised hyperparameters and saves the parameters of the best model.\n",
    "    \"\"\"\n",
    "    \n",
    "  def __init__(self, \n",
    "               model, \n",
    "               train_loader, \n",
    "               test_loader,\n",
    "               criterion,\n",
    "               num_epochs,\n",
    "               n_trials,\n",
    "               study_name):\n",
    "    self.model = model\n",
    "    self.train_loader = train_loader\n",
    "    self.test_loader = test_loader\n",
    "    self.criterion = criterion\n",
    "    self.num_epochs = num_epochs\n",
    "    self.n_trials = n_trials\n",
    "    self.study_name = study_name\n",
    "    self.len_train_loader = len(train_loader['FFNN'])\n",
    "    self.len_test_loader = len(test_loader['FFNN'])\n",
    "    self.best_model = None\n",
    "\n",
    "  def objective(self, trial):\n",
    "    \"\"\"Defines the objective to be optimised (F1 test score) and saves\n",
    "    each final model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate the model\n",
    "    model = self.model\n",
    "\n",
    "    # generate the possible optimizers\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\"])\n",
    "    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    \n",
    "    # convert model data type to double\n",
    "    model = model.double()\n",
    "    \n",
    "    # Define the training and testing phases\n",
    "    for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "      train_loss = 0.0\n",
    "      test_loss = 0.0\n",
    "      f1_test = 0.0\n",
    "    \n",
    "      # set the model in training modality\n",
    "      model.train()\n",
    "      for load1, load2 in tqdm(zip(self.train_loader['FFNN'],\n",
    "                                          self.train_loader['CNN'])) \n",
    "                                      desc='Training model', total = self.len_train_loader):\n",
    "        x_1, target = load1\n",
    "        x_2, _ = load2\n",
    "\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model([x_1.double(), x_2.double()])\n",
    "        # calculate the batch loss as a sum of the single losses\n",
    "        loss = self.criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss wrt model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()      \n",
    "        \n",
    "        \n",
    "      # set the model in testing modality\n",
    "      model.eval()\n",
    "      for load1, load2 in tqdm(zip(self.test_loader['FFNN'],\n",
    "                                          self.test_loader['CNN']), \n",
    "                                      desc='Testing model', total = self.len_test_loader):\n",
    "        x_1, target = load1\n",
    "        x_2, _, = load2\n",
    "\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output1, output2 = model([x_1.double(), x_2.double()])\n",
    "        # calculate the batch loss as a sum of the single losses\n",
    "        loss = self.criterion(output, target)\n",
    "        # update test loss \n",
    "        test_loss += loss.item() \n",
    "        # calculate F1 test score as weighted sum of the single F1 scores\n",
    "        f1_test += F1(output,target) \n",
    "        \n",
    "      # calculate epoch score by dividing by the number of observations\n",
    "      f1_test /= self.len_test_loader\n",
    "      # pass the score of the epoch to the study to monitor the intermediate objective values    \n",
    "      trial.report(f1_test, epoch)\n",
    "\n",
    "    # save the final model named with the number of the trial \n",
    "    with open(\"{}{}.pickle\".format(self.study_name,trial.number), \"wb\") as fout:\n",
    "      pickle.dump(model, fout)\n",
    "    \n",
    "    # return F1 score to the study        \n",
    "    return f1_test\n",
    "\n",
    "\n",
    "\n",
    "  def run_trial(self):\n",
    "    \"\"\"Runs Optuna study and stores the best model in class attribute 'best_model'.\"\"\"\n",
    "\n",
    "    # create a new study or load a pre-existing study. use sqlite backend to store the study.\n",
    "    study = optuna.create_study(study_name=self.study_name, direction=\"maximize\", \n",
    "                                storage='sqlite:///SA_optuna_tuning.db', load_if_exists=True)\n",
    "\n",
    "    complete_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.COMPLETE]\n",
    "    pruned_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.PRUNED]\n",
    "    \n",
    "    # if the number of already completed trials is lower than the total number of trials passed as\n",
    "    #argument, perform the remaining trials \n",
    "    if len(complete_trials)<self.n_trials:\n",
    "        # set the number of trials to be performed equal to the number of missing trials\n",
    "        self.n_trials -= len(complete_trials)\n",
    "        study.optimize(self.objective, n_trials=self.n_trials)\n",
    "        pruned_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.PRUNED]\n",
    "        complete_trials = [t for t in study.trials if t.state == optuna.structs.TrialState.COMPLETE]\n",
    "        \n",
    "    # store the best model found in the class\n",
    "    with open(\"{}{}.pickle\".format(self.study_name, study.best_trial.number), \"rb\") as fin:\n",
    "        best_model = pickle.load(fin)\n",
    "\n",
    "    self.best_model = best_model\n",
    "    \n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "      print(\"    {}: {}\".format(key, value))\n",
    "                                          \n",
    "\n",
    "    with open(\"{}.pickle\".format(study.best_trial.number), \"rb\") as fin:\n",
    "      best_model = pickle.load(fin)\n",
    "    \n",
    "    # store only best model\n",
    "    self.best_model = best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwpyKxqQexj1"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def fit_multimodal(model, \n",
    "                   train_loader, \n",
    "                   test_loader, \n",
    "                   criterion, \n",
    "                   optimizer, \n",
    "                   num_epochs, \n",
    "                   filename_path, \n",
    "                   verbose=True): \n",
    "  \"\"\"Performs the training of the multitask model. It implements also early stopping\n",
    "    \n",
    "    Parameters:\n",
    "    ------------------\n",
    "    model (torch.nn.Module): neural network model.\n",
    "    train_loader (DataLoader): dictioary of training DataLoader objects. Keys of the\n",
    "        dictionary must be 'FFNN', 'CNN', 'D2V_CNN'.\n",
    "    test_loader (DataLoader): dictionary of testing DataLoader objects. Keys of the\n",
    "        dictionary must be 'FFNN', 'CNN', 'D2V_CNN'.\n",
    "    criterion: loss function for training the model.\n",
    "    optimizer (torch.optim): optimization algorithm for training the model. \n",
    "    num_epochs (int): number of epochs.\n",
    "    filename_path (str): where the weights of the model at each epoch will be stored. \n",
    "        Indicate only the name of the folder.\n",
    "    patience (int): number of epochs in which the test error is not anymore decreasing\n",
    "        before stopping the training.\n",
    "    delta (int): minimum decrease in the test error to continue with the training.\n",
    "        Default:0\n",
    "    verbose (bool): prints the training error, test error, F1 training score, F1 test score \n",
    "        at each epoch.\n",
    "        Default: True\n",
    "    \n",
    "    Attributes:\n",
    "    ------------------\n",
    "    f1_train_scores: stores the F1 training scores for each epoch.\n",
    "    f1_test_scores: stores the F1 test scores for each epoch.\n",
    "    \n",
    "    Returns:\n",
    "    ------------------\n",
    "    Lists of F1 training scores and F1 test scores at each epoch.\n",
    "    Prints training error, test error, F1 training score, F1 test score at each epoch.\n",
    "    \"\"\"\n",
    "\n",
    "  basepath = 'exp'\n",
    "\n",
    "  # keep track of epoch losses \n",
    "  f1_train_scores = []\n",
    "  f1_test_scores = []\n",
    "\n",
    "  # convert model data type to double\n",
    "  model = model.double()\n",
    "\n",
    "  # define early stopping\n",
    "  early_stopping = EarlyStopping(patience=patience, delta=delta, verbose=True)\n",
    "\n",
    "  len_train_loader = len(train_loader['FFNN'])\n",
    "  len_test_loader = len(test_loader['FFNN'])\n",
    "    \n",
    "\n",
    "  for epoch in tqdm(range(1, num_epochs + 1), desc='Epochs'):\n",
    "    train_loss = 0.0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    f1_train = 0.0\n",
    "    f1_test = 0.0\n",
    "    \n",
    "    # if there is already a trained model stored for a specific epoch, load the model\n",
    "    #and don't retrain the model\n",
    "    PATH = os.path.join(basepath, filename_path + '_' + str(epoch) + '.pt')\n",
    "    if os.path.exists(PATH):\n",
    "      checkpoint = torch.load(PATH)\n",
    "      model.load_state_dict(checkpoint['model_state_dict'])\n",
    "      f1_train = checkpoint['F1_train']\n",
    "      f1_test = checkpoint['F1_test']\n",
    "      train_loss = checkpoint['train_loss']\n",
    "      test_loss = checkpoint['test_loss']\n",
    "        \n",
    "    else:\n",
    "      # set the model in training modality\n",
    "      model.train()\n",
    "      for load1, load2, load3 in tqdm(zip(train_loader['FFNN'],\n",
    "                                          train_loader['CNN']), \n",
    "                                      desc='Testing model', total = len_train_loader):\n",
    "        x_1, target = load1\n",
    "        x_2, _ = load2\n",
    "        x_3, _ = load3\n",
    "        \n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model([x_1.double(), x_2.double()])\n",
    "        # calculate the batch loss as the sum of all the losses\n",
    "        loss = criterion(output1 target) \n",
    "        # backward pass: compute gradient of the loss wrt model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()\n",
    "        # calculate F1 training score as a weighted sum of the single F1 scores\n",
    "        f1_train +=  F1(output,target) \n",
    "        \n",
    "        \n",
    "      # set the model in testing modality\n",
    "      model.eval()\n",
    "      for load1, load2, load3 in tqdm(zip(test_loader['FFNN'],\n",
    "                                          test_loader['CNN']), \n",
    "                                      desc='Testing model', total = len_test_loader):\n",
    "        x_1, target = load1\n",
    "        x_2, _ = load2\n",
    "        x_3, _ = load3\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model([x_1.double(), x_2.double()])\n",
    "        # calculate the batch loss as the sum of all the losses\n",
    "        loss = criterion(output, target) \n",
    "        # update test loss\n",
    "        test_loss += loss.item()\n",
    "        # calculate F1 test score as a weighted sum of the single F1 scores\n",
    "        f1_test +=  F1(output,target) \n",
    "        \n",
    "        \n",
    "    # save the model weights, epoch, scores and losses at each epoch\n",
    "    model_param = model.state_dict()\n",
    "    PATH = os.path.join(basepath, filename_path + '_' + str(epoch) + '.pt')\n",
    "    torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model_param,\n",
    "                'F1_train': f1_train,\n",
    "                'F1_test': f1_test,\n",
    "                'train_loss': train_loss,\n",
    "                'test_loss': test_loss},\n",
    "               PATH)\n",
    "     \n",
    "    \n",
    "    # calculate epoch score by dividing by the number of observations\n",
    "    f1_train /= len_train_loader\n",
    "    f1_test /= len_test_loader\n",
    "    # store epoch scores\n",
    "    f1_train_scores.append(f1_train)    \n",
    "    f1_test_scores.append(f1_test)\n",
    "      \n",
    "    # print training/test statistics \n",
    "    if verbose == True:\n",
    "      print('Epoch: {} \\tTraining F1 score: {:.4f} \\tTest F1 score: {:.4f} \\tTraining Loss: {:.4f} \\tTest Loss: {:.4f}'.format(\n",
    "      epoch, f1_train, f1_test, train_loss, test_loss))\n",
    "      \n",
    "    # early stop the model if the test loss is not improving\n",
    "    early_stopping(test_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "      print('Early stopping the training')\n",
    "      # reload the previous best model before the test loss started decreasing\n",
    "      best_checkpoint = torch.load(os.path.join(basepath,filename_path + '_' + '{}'.format(epoch-patience) + '.pt'))\n",
    "      model.load_state_dict(best_checkpoint['model_state_dict'])\n",
    "      break\n",
    "    \n",
    "        \n",
    "  # return the scores at each epoch\n",
    "  return f1_train_scores, f1_test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtReJYQW7bBj"
   },
   "source": [
    "## Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDe4BnP_7eEw"
   },
   "source": [
    "### Model testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "02_Thesis_BIOINF_multimodal_NN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0266ac46c9b9453a9539ea39e6710364": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0bb7a6aef6f344068b1a1ef798debead": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b6812b69f4f840f0b2f733b077e83919",
      "placeholder": "​",
      "style": "IPY_MODEL_0266ac46c9b9453a9539ea39e6710364",
      "value": " 405/? [01:19&lt;00:00,  5.12it/s]"
     }
    },
    "131a0ead97934369875db3b372dc2a2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "1726012577934d22976d0d887c1e9550": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1b704b6614534e2286d7203527c1c216": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a5f107767969497fb1980cc0ab6262b0",
       "IPY_MODEL_7b802877c3dc4c759de9a0bfc10ff400"
      ],
      "layout": "IPY_MODEL_a68fcc18c0f64e0f9bf6d3002bcab74d"
     }
    },
    "1e7aad6fee9c4a78a819560cf7df65d1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2274bbb4d74148b2bf414c324b406965": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "316a7ea0a7af45a18856c3e17d3f8565": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "35b5608b0dc44af080ce58a099cd8712": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "51871383359e47c5be7e7ed3763f8024": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "52224afa7ed04cd59f64a33a124f15f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "6172d0db84ac405d90a81ca17e031d5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Training model: ",
      "description_tooltip": null,
      "layout": "IPY_MODEL_51871383359e47c5be7e7ed3763f8024",
      "max": 404,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_52224afa7ed04cd59f64a33a124f15f8",
      "value": 404
     }
    },
    "61e41656f25e4a31b2fa8b673df87c31": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "7b802877c3dc4c759de9a0bfc10ff400": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2274bbb4d74148b2bf414c324b406965",
      "placeholder": "​",
      "style": "IPY_MODEL_8a4f2ba83b2a4291a5b332d025200abb",
      "value": " 37/? [14:25&lt;00:00, 23.40s/it]"
     }
    },
    "87899c5edd9342d2ac4b4ccc85ffc239": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e7aad6fee9c4a78a819560cf7df65d1",
      "placeholder": "​",
      "style": "IPY_MODEL_1726012577934d22976d0d887c1e9550",
      "value": " 0/2 [01:32&lt;?, ?it/s]"
     }
    },
    "8a4f2ba83b2a4291a5b332d025200abb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8fb7a0d167284bf79ff068a5aa85e9d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "danger",
      "description": "Epochs:   0%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc5de6784a954a5987ec7b2e85238215",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_61e41656f25e4a31b2fa8b673df87c31",
      "value": 0
     }
    },
    "a5f107767969497fb1980cc0ab6262b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Testing model: ",
      "description_tooltip": null,
      "layout": "IPY_MODEL_316a7ea0a7af45a18856c3e17d3f8565",
      "max": 36,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_131a0ead97934369875db3b372dc2a2a",
      "value": 36
     }
    },
    "a68fcc18c0f64e0f9bf6d3002bcab74d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6d85f1038cd41da900f5edc0e80811d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8fb7a0d167284bf79ff068a5aa85e9d2",
       "IPY_MODEL_87899c5edd9342d2ac4b4ccc85ffc239"
      ],
      "layout": "IPY_MODEL_35b5608b0dc44af080ce58a099cd8712"
     }
    },
    "b6812b69f4f840f0b2f733b077e83919": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc5de6784a954a5987ec7b2e85238215": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1f8579b4d1b47b08269c9a8fbea17be": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6172d0db84ac405d90a81ca17e031d5e",
       "IPY_MODEL_0bb7a6aef6f344068b1a1ef798debead"
      ],
      "layout": "IPY_MODEL_f589903ecd1448d6a0eab7fce3b822f2"
     }
    },
    "f589903ecd1448d6a0eab7fce3b822f2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
